{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练胜负模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2331208/2835855775.py:310: DtypeWarning: Columns (37,74,111,148,185,222,259,296,333,370,407,444,481,495,496,497,532,533,534,535,570,571,572,573,608,609,610,611,646,647,648,649,684,721,758,795,832,869,906,943,980,1017,1054,1091,1128,1165,1179,1180,1181,1216,1217,1218,1219,1254,1255,1256,1257,1292,1293,1294,1295,1330,1331,1332,1333,1368) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 99.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63162 92 63254\n",
      "清洗后数据： 63204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2331208/2835855775.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[time_features] = df[time_features].fillna('0:0')\n",
      "/tmp/ipykernel_2331208/2835855775.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[time_features] = df[time_features].map(lambda x: '0:0' if ':' not in str(x) else x)\n",
      "/tmp/ipykernel_2331208/2835855775.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[time_features] =df[time_features].map(lambda x: float(x.replace(':', '.')))\n",
      "/tmp/ipykernel_2331208/2835855775.py:51: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n",
      "/tmp/ipykernel_2331208/2835855775.py:63: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2275344, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvp_shap/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2331208/2835855775.py:81: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2275344, 1)\n",
      "    DRtg_0  DRtg_35\n",
      "0      3.0      0.0\n",
      "1      4.0      0.0\n",
      "2      1.0      0.0\n",
      "3      3.0      0.0\n",
      "4      4.0      0.0\n",
      "..     ...      ...\n",
      "87     4.0      0.0\n",
      "88     4.0      0.0\n",
      "89     4.0      0.0\n",
      "90     2.0      0.0\n",
      "91     1.0      0.0\n",
      "\n",
      "[63204 rows x 2 columns]\n",
      "    +/-_0  +/-_35\n",
      "0     0.0     0.0\n",
      "1     0.0     0.0\n",
      "2     0.0     0.0\n",
      "3     0.0     0.0\n",
      "4     0.0     0.0\n",
      "..    ...     ...\n",
      "87    0.0     0.0\n",
      "88    0.0     0.0\n",
      "89    0.0     0.0\n",
      "90    0.0     0.0\n",
      "91    0.0     0.0\n",
      "\n",
      "[63204 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvp_shap/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63204 entries, 0 to 91\n",
      "Columns: 1372 entries, Unnamed: 0 to result\n",
      "dtypes: float64(1047), int64(193), object(132)\n",
      "memory usage: 662.1+ MB\n",
      "None\n",
      "Index(['Unnamed: 0', 'starters_0', 'team_0', 'MP_0', 'FG_0', 'FGA_0', 'FG%_0',\n",
      "       '3P_0', '3PA_0', '3P%_0',\n",
      "       ...\n",
      "       'BLK%_35', 'TOV%_35', 'USG%_35', 'ORtg_35', 'DRtg_35', 'BPM_35',\n",
      "       'file_name_35', 'score_ourside', 'score_opposite', 'result'],\n",
      "      dtype='object', length=1372)\n",
      "选择的特征数量： 1260\n",
      "处理后数据： 63204\n",
      "['DRtg_0', 'DRtg_1', 'DRtg_2', 'DRtg_3', 'DRtg_4', 'DRtg_5', 'DRtg_6', 'DRtg_7', 'DRtg_8', 'DRtg_9', 'DRtg_10', 'DRtg_11', 'DRtg_12', 'DRtg_13', 'DRtg_14', 'DRtg_15', 'DRtg_16', 'DRtg_17', 'DRtg_18', 'DRtg_19', 'DRtg_20', 'DRtg_21', 'DRtg_22', 'DRtg_23', 'DRtg_24', 'DRtg_25', 'DRtg_26', 'DRtg_27', 'DRtg_28', 'DRtg_29', 'DRtg_30', 'DRtg_31', 'DRtg_32', 'DRtg_33', 'DRtg_34', 'DRtg_35', '+/-_0', '+/-_1', '+/-_2', '+/-_3', '+/-_4', '+/-_5', '+/-_6', '+/-_7', '+/-_8', '+/-_9', '+/-_10', '+/-_11', '+/-_12', '+/-_13', '+/-_14', '+/-_15', '+/-_16', '+/-_17', '+/-_18', '+/-_19', '+/-_20', '+/-_21', '+/-_22', '+/-_23', '+/-_24', '+/-_25', '+/-_26', '+/-_27', '+/-_28', '+/-_29', '+/-_30', '+/-_31', '+/-_32', '+/-_33', '+/-_34', '+/-_35']\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 25238, number of negative: 25325\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 93898\n",
      "[LightGBM] [Info] Number of data points in the train set: 50563, number of used features: 926\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499140 -> initscore=-0.003441\n",
      "[LightGBM] [Info] Start training from score -0.003441\n",
      "[1]\tvalid_0's binary_logloss: 0.627751\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.572696\n",
      "[3]\tvalid_0's binary_logloss: 0.525417\n",
      "[4]\tvalid_0's binary_logloss: 0.484585\n",
      "[5]\tvalid_0's binary_logloss: 0.448587\n",
      "[6]\tvalid_0's binary_logloss: 0.417159\n",
      "[7]\tvalid_0's binary_logloss: 0.388978\n",
      "[8]\tvalid_0's binary_logloss: 0.364066\n",
      "[9]\tvalid_0's binary_logloss: 0.341423\n",
      "[10]\tvalid_0's binary_logloss: 0.321368\n",
      "[11]\tvalid_0's binary_logloss: 0.303355\n",
      "[12]\tvalid_0's binary_logloss: 0.28673\n",
      "[13]\tvalid_0's binary_logloss: 0.271608\n",
      "[14]\tvalid_0's binary_logloss: 0.257682\n",
      "[15]\tvalid_0's binary_logloss: 0.245121\n",
      "[16]\tvalid_0's binary_logloss: 0.233741\n",
      "[17]\tvalid_0's binary_logloss: 0.223346\n",
      "[18]\tvalid_0's binary_logloss: 0.213593\n",
      "[19]\tvalid_0's binary_logloss: 0.204782\n",
      "[20]\tvalid_0's binary_logloss: 0.196631\n",
      "[21]\tvalid_0's binary_logloss: 0.189023\n",
      "[22]\tvalid_0's binary_logloss: 0.18214\n",
      "[23]\tvalid_0's binary_logloss: 0.175567\n",
      "[24]\tvalid_0's binary_logloss: 0.16964\n",
      "[25]\tvalid_0's binary_logloss: 0.164383\n",
      "[26]\tvalid_0's binary_logloss: 0.159071\n",
      "[27]\tvalid_0's binary_logloss: 0.154324\n",
      "[28]\tvalid_0's binary_logloss: 0.149792\n",
      "[29]\tvalid_0's binary_logloss: 0.145703\n",
      "[30]\tvalid_0's binary_logloss: 0.141827\n",
      "[31]\tvalid_0's binary_logloss: 0.138371\n",
      "[32]\tvalid_0's binary_logloss: 0.135008\n",
      "[33]\tvalid_0's binary_logloss: 0.131604\n",
      "[34]\tvalid_0's binary_logloss: 0.128626\n",
      "[35]\tvalid_0's binary_logloss: 0.125753\n",
      "[36]\tvalid_0's binary_logloss: 0.123295\n",
      "[37]\tvalid_0's binary_logloss: 0.120728\n",
      "[38]\tvalid_0's binary_logloss: 0.118372\n",
      "[39]\tvalid_0's binary_logloss: 0.116319\n",
      "[40]\tvalid_0's binary_logloss: 0.114455\n",
      "[41]\tvalid_0's binary_logloss: 0.112417\n",
      "[42]\tvalid_0's binary_logloss: 0.110553\n",
      "[43]\tvalid_0's binary_logloss: 0.108665\n",
      "[44]\tvalid_0's binary_logloss: 0.106992\n",
      "[45]\tvalid_0's binary_logloss: 0.105502\n",
      "[46]\tvalid_0's binary_logloss: 0.104047\n",
      "[47]\tvalid_0's binary_logloss: 0.102852\n",
      "[48]\tvalid_0's binary_logloss: 0.101325\n",
      "[49]\tvalid_0's binary_logloss: 0.100098\n",
      "[50]\tvalid_0's binary_logloss: 0.098858\n",
      "[51]\tvalid_0's binary_logloss: 0.0978378\n",
      "[52]\tvalid_0's binary_logloss: 0.096829\n",
      "[53]\tvalid_0's binary_logloss: 0.0957611\n",
      "[54]\tvalid_0's binary_logloss: 0.0947812\n",
      "[55]\tvalid_0's binary_logloss: 0.0937466\n",
      "[56]\tvalid_0's binary_logloss: 0.0927046\n",
      "[57]\tvalid_0's binary_logloss: 0.091812\n",
      "[58]\tvalid_0's binary_logloss: 0.0911147\n",
      "[59]\tvalid_0's binary_logloss: 0.0901787\n",
      "[60]\tvalid_0's binary_logloss: 0.0893374\n",
      "[61]\tvalid_0's binary_logloss: 0.0886026\n",
      "[62]\tvalid_0's binary_logloss: 0.0879008\n",
      "[63]\tvalid_0's binary_logloss: 0.0871281\n",
      "[64]\tvalid_0's binary_logloss: 0.0864622\n",
      "[65]\tvalid_0's binary_logloss: 0.0857429\n",
      "[66]\tvalid_0's binary_logloss: 0.0851642\n",
      "[67]\tvalid_0's binary_logloss: 0.0846703\n",
      "[68]\tvalid_0's binary_logloss: 0.084207\n",
      "[69]\tvalid_0's binary_logloss: 0.0837435\n",
      "[70]\tvalid_0's binary_logloss: 0.0832103\n",
      "[71]\tvalid_0's binary_logloss: 0.0827465\n",
      "[72]\tvalid_0's binary_logloss: 0.0823001\n",
      "[73]\tvalid_0's binary_logloss: 0.0818433\n",
      "[74]\tvalid_0's binary_logloss: 0.0814396\n",
      "[75]\tvalid_0's binary_logloss: 0.0810002\n",
      "[76]\tvalid_0's binary_logloss: 0.0807077\n",
      "[77]\tvalid_0's binary_logloss: 0.0804137\n",
      "[78]\tvalid_0's binary_logloss: 0.0800127\n",
      "[79]\tvalid_0's binary_logloss: 0.079566\n",
      "[80]\tvalid_0's binary_logloss: 0.0792577\n",
      "[81]\tvalid_0's binary_logloss: 0.0788884\n",
      "[82]\tvalid_0's binary_logloss: 0.0785316\n",
      "[83]\tvalid_0's binary_logloss: 0.0780334\n",
      "[84]\tvalid_0's binary_logloss: 0.0777968\n",
      "[85]\tvalid_0's binary_logloss: 0.0775002\n",
      "[86]\tvalid_0's binary_logloss: 0.0771508\n",
      "[87]\tvalid_0's binary_logloss: 0.0768865\n",
      "[88]\tvalid_0's binary_logloss: 0.0766829\n",
      "[89]\tvalid_0's binary_logloss: 0.0764474\n",
      "[90]\tvalid_0's binary_logloss: 0.0760715\n",
      "[91]\tvalid_0's binary_logloss: 0.0757682\n",
      "[92]\tvalid_0's binary_logloss: 0.0754788\n",
      "[93]\tvalid_0's binary_logloss: 0.0754698\n",
      "[94]\tvalid_0's binary_logloss: 0.0752895\n",
      "[95]\tvalid_0's binary_logloss: 0.0749872\n",
      "[96]\tvalid_0's binary_logloss: 0.074713\n",
      "[97]\tvalid_0's binary_logloss: 0.0744082\n",
      "[98]\tvalid_0's binary_logloss: 0.0742558\n",
      "[99]\tvalid_0's binary_logloss: 0.0740754\n",
      "[100]\tvalid_0's binary_logloss: 0.073773\n",
      "[101]\tvalid_0's binary_logloss: 0.0735357\n",
      "[102]\tvalid_0's binary_logloss: 0.0733829\n",
      "[103]\tvalid_0's binary_logloss: 0.0731078\n",
      "[104]\tvalid_0's binary_logloss: 0.0729405\n",
      "[105]\tvalid_0's binary_logloss: 0.0725857\n",
      "[106]\tvalid_0's binary_logloss: 0.0724689\n",
      "[107]\tvalid_0's binary_logloss: 0.0722712\n",
      "[108]\tvalid_0's binary_logloss: 0.0720759\n",
      "[109]\tvalid_0's binary_logloss: 0.0718412\n",
      "[110]\tvalid_0's binary_logloss: 0.0716366\n",
      "[111]\tvalid_0's binary_logloss: 0.0715321\n",
      "[112]\tvalid_0's binary_logloss: 0.0713814\n",
      "[113]\tvalid_0's binary_logloss: 0.0712453\n",
      "[114]\tvalid_0's binary_logloss: 0.0711475\n",
      "[115]\tvalid_0's binary_logloss: 0.0710606\n",
      "[116]\tvalid_0's binary_logloss: 0.0708221\n",
      "[117]\tvalid_0's binary_logloss: 0.0706517\n",
      "[118]\tvalid_0's binary_logloss: 0.0705383\n",
      "[119]\tvalid_0's binary_logloss: 0.070368\n",
      "[120]\tvalid_0's binary_logloss: 0.0701809\n",
      "[121]\tvalid_0's binary_logloss: 0.0700498\n",
      "[122]\tvalid_0's binary_logloss: 0.0699647\n",
      "[123]\tvalid_0's binary_logloss: 0.0699532\n",
      "[124]\tvalid_0's binary_logloss: 0.069927\n",
      "[125]\tvalid_0's binary_logloss: 0.0697406\n",
      "[126]\tvalid_0's binary_logloss: 0.0695233\n",
      "[127]\tvalid_0's binary_logloss: 0.0693472\n",
      "[128]\tvalid_0's binary_logloss: 0.0693398\n",
      "[129]\tvalid_0's binary_logloss: 0.0692665\n",
      "[130]\tvalid_0's binary_logloss: 0.0691055\n",
      "[131]\tvalid_0's binary_logloss: 0.0688644\n",
      "[132]\tvalid_0's binary_logloss: 0.0687138\n",
      "[133]\tvalid_0's binary_logloss: 0.068603\n",
      "[134]\tvalid_0's binary_logloss: 0.0684402\n",
      "[135]\tvalid_0's binary_logloss: 0.0684166\n",
      "[136]\tvalid_0's binary_logloss: 0.0682712\n",
      "[137]\tvalid_0's binary_logloss: 0.0681807\n",
      "[138]\tvalid_0's binary_logloss: 0.0680903\n",
      "[139]\tvalid_0's binary_logloss: 0.0680499\n",
      "[140]\tvalid_0's binary_logloss: 0.0679355\n",
      "[141]\tvalid_0's binary_logloss: 0.067858\n",
      "[142]\tvalid_0's binary_logloss: 0.0677733\n",
      "[143]\tvalid_0's binary_logloss: 0.0676511\n",
      "[144]\tvalid_0's binary_logloss: 0.0676665\n",
      "[145]\tvalid_0's binary_logloss: 0.0675205\n",
      "[146]\tvalid_0's binary_logloss: 0.0673562\n",
      "[147]\tvalid_0's binary_logloss: 0.0673012\n",
      "[148]\tvalid_0's binary_logloss: 0.0672295\n",
      "[149]\tvalid_0's binary_logloss: 0.0671127\n",
      "[150]\tvalid_0's binary_logloss: 0.0670452\n",
      "[151]\tvalid_0's binary_logloss: 0.0668518\n",
      "[152]\tvalid_0's binary_logloss: 0.0667618\n",
      "[153]\tvalid_0's binary_logloss: 0.0666388\n",
      "[154]\tvalid_0's binary_logloss: 0.0664773\n",
      "[155]\tvalid_0's binary_logloss: 0.0663951\n",
      "[156]\tvalid_0's binary_logloss: 0.0662669\n",
      "[157]\tvalid_0's binary_logloss: 0.0661304\n",
      "[158]\tvalid_0's binary_logloss: 0.0660533\n",
      "[159]\tvalid_0's binary_logloss: 0.0660254\n",
      "[160]\tvalid_0's binary_logloss: 0.0659472\n",
      "[161]\tvalid_0's binary_logloss: 0.065863\n",
      "[162]\tvalid_0's binary_logloss: 0.0656915\n",
      "[163]\tvalid_0's binary_logloss: 0.0656228\n",
      "[164]\tvalid_0's binary_logloss: 0.0655023\n",
      "[165]\tvalid_0's binary_logloss: 0.0654288\n",
      "[166]\tvalid_0's binary_logloss: 0.0654069\n",
      "[167]\tvalid_0's binary_logloss: 0.0653143\n",
      "[168]\tvalid_0's binary_logloss: 0.0652999\n",
      "[169]\tvalid_0's binary_logloss: 0.0652412\n",
      "[170]\tvalid_0's binary_logloss: 0.0651142\n",
      "[171]\tvalid_0's binary_logloss: 0.0650789\n",
      "[172]\tvalid_0's binary_logloss: 0.0650082\n",
      "[173]\tvalid_0's binary_logloss: 0.0650038\n",
      "[174]\tvalid_0's binary_logloss: 0.06495\n",
      "[175]\tvalid_0's binary_logloss: 0.0647851\n",
      "[176]\tvalid_0's binary_logloss: 0.064679\n",
      "[177]\tvalid_0's binary_logloss: 0.0646133\n",
      "[178]\tvalid_0's binary_logloss: 0.0646019\n",
      "[179]\tvalid_0's binary_logloss: 0.0645091\n",
      "[180]\tvalid_0's binary_logloss: 0.0644806\n",
      "[181]\tvalid_0's binary_logloss: 0.0644408\n",
      "[182]\tvalid_0's binary_logloss: 0.0643832\n",
      "[183]\tvalid_0's binary_logloss: 0.0643347\n",
      "[184]\tvalid_0's binary_logloss: 0.064299\n",
      "[185]\tvalid_0's binary_logloss: 0.0642315\n",
      "[186]\tvalid_0's binary_logloss: 0.0642286\n",
      "[187]\tvalid_0's binary_logloss: 0.0641601\n",
      "[188]\tvalid_0's binary_logloss: 0.064074\n",
      "[189]\tvalid_0's binary_logloss: 0.0639642\n",
      "[190]\tvalid_0's binary_logloss: 0.0639594\n",
      "[191]\tvalid_0's binary_logloss: 0.0639411\n",
      "[192]\tvalid_0's binary_logloss: 0.0638519\n",
      "[193]\tvalid_0's binary_logloss: 0.0638052\n",
      "[194]\tvalid_0's binary_logloss: 0.0637137\n",
      "[195]\tvalid_0's binary_logloss: 0.0635902\n",
      "[196]\tvalid_0's binary_logloss: 0.0634619\n",
      "[197]\tvalid_0's binary_logloss: 0.0633537\n",
      "[198]\tvalid_0's binary_logloss: 0.0632712\n",
      "[199]\tvalid_0's binary_logloss: 0.0631545\n",
      "[200]\tvalid_0's binary_logloss: 0.0630391\n",
      "[201]\tvalid_0's binary_logloss: 0.0630037\n",
      "[202]\tvalid_0's binary_logloss: 0.0629089\n",
      "[203]\tvalid_0's binary_logloss: 0.062856\n",
      "[204]\tvalid_0's binary_logloss: 0.0627872\n",
      "[205]\tvalid_0's binary_logloss: 0.0627003\n",
      "[206]\tvalid_0's binary_logloss: 0.0626292\n",
      "[207]\tvalid_0's binary_logloss: 0.0625922\n",
      "[208]\tvalid_0's binary_logloss: 0.0624919\n",
      "[209]\tvalid_0's binary_logloss: 0.0624418\n",
      "[210]\tvalid_0's binary_logloss: 0.0623558\n",
      "[211]\tvalid_0's binary_logloss: 0.0623193\n",
      "[212]\tvalid_0's binary_logloss: 0.06225\n",
      "[213]\tvalid_0's binary_logloss: 0.0621796\n",
      "[214]\tvalid_0's binary_logloss: 0.0621412\n",
      "[215]\tvalid_0's binary_logloss: 0.0620682\n",
      "[216]\tvalid_0's binary_logloss: 0.0619923\n",
      "[217]\tvalid_0's binary_logloss: 0.061908\n",
      "[218]\tvalid_0's binary_logloss: 0.061795\n",
      "[219]\tvalid_0's binary_logloss: 0.0617654\n",
      "[220]\tvalid_0's binary_logloss: 0.0616603\n",
      "[221]\tvalid_0's binary_logloss: 0.0615688\n",
      "[222]\tvalid_0's binary_logloss: 0.0615331\n",
      "[223]\tvalid_0's binary_logloss: 0.0614848\n",
      "[224]\tvalid_0's binary_logloss: 0.0614631\n",
      "[225]\tvalid_0's binary_logloss: 0.0614276\n",
      "[226]\tvalid_0's binary_logloss: 0.0614335\n",
      "[227]\tvalid_0's binary_logloss: 0.0613513\n",
      "[228]\tvalid_0's binary_logloss: 0.0612821\n",
      "[229]\tvalid_0's binary_logloss: 0.0612129\n",
      "[230]\tvalid_0's binary_logloss: 0.0610985\n",
      "[231]\tvalid_0's binary_logloss: 0.0610464\n",
      "[232]\tvalid_0's binary_logloss: 0.0610128\n",
      "[233]\tvalid_0's binary_logloss: 0.0610347\n",
      "[234]\tvalid_0's binary_logloss: 0.0609484\n",
      "[235]\tvalid_0's binary_logloss: 0.0609111\n",
      "[236]\tvalid_0's binary_logloss: 0.0608637\n",
      "[237]\tvalid_0's binary_logloss: 0.0607969\n",
      "[238]\tvalid_0's binary_logloss: 0.0607941\n",
      "[239]\tvalid_0's binary_logloss: 0.0607482\n",
      "[240]\tvalid_0's binary_logloss: 0.0606921\n",
      "[241]\tvalid_0's binary_logloss: 0.0606474\n",
      "[242]\tvalid_0's binary_logloss: 0.0606593\n",
      "[243]\tvalid_0's binary_logloss: 0.0606178\n",
      "[244]\tvalid_0's binary_logloss: 0.0606409\n",
      "[245]\tvalid_0's binary_logloss: 0.0605836\n",
      "[246]\tvalid_0's binary_logloss: 0.0606439\n",
      "[247]\tvalid_0's binary_logloss: 0.0605685\n",
      "[248]\tvalid_0's binary_logloss: 0.0605607\n",
      "[249]\tvalid_0's binary_logloss: 0.0604826\n",
      "[250]\tvalid_0's binary_logloss: 0.0604553\n",
      "[251]\tvalid_0's binary_logloss: 0.0603953\n",
      "[252]\tvalid_0's binary_logloss: 0.060329\n",
      "[253]\tvalid_0's binary_logloss: 0.0602974\n",
      "[254]\tvalid_0's binary_logloss: 0.0602658\n",
      "[255]\tvalid_0's binary_logloss: 0.0601712\n",
      "[256]\tvalid_0's binary_logloss: 0.0601637\n",
      "[257]\tvalid_0's binary_logloss: 0.0600948\n",
      "[258]\tvalid_0's binary_logloss: 0.0600537\n",
      "[259]\tvalid_0's binary_logloss: 0.0600159\n",
      "[260]\tvalid_0's binary_logloss: 0.0599663\n",
      "[261]\tvalid_0's binary_logloss: 0.0599572\n",
      "[262]\tvalid_0's binary_logloss: 0.0600372\n",
      "[263]\tvalid_0's binary_logloss: 0.0600474\n",
      "[264]\tvalid_0's binary_logloss: 0.0599891\n",
      "[265]\tvalid_0's binary_logloss: 0.059952\n",
      "[266]\tvalid_0's binary_logloss: 0.059925\n",
      "[267]\tvalid_0's binary_logloss: 0.0599275\n",
      "[268]\tvalid_0's binary_logloss: 0.0598285\n",
      "[269]\tvalid_0's binary_logloss: 0.059821\n",
      "[270]\tvalid_0's binary_logloss: 0.0598169\n",
      "[271]\tvalid_0's binary_logloss: 0.0597242\n",
      "[272]\tvalid_0's binary_logloss: 0.0596818\n",
      "[273]\tvalid_0's binary_logloss: 0.0596651\n",
      "[274]\tvalid_0's binary_logloss: 0.0596221\n",
      "[275]\tvalid_0's binary_logloss: 0.059621\n",
      "[276]\tvalid_0's binary_logloss: 0.0595868\n",
      "[277]\tvalid_0's binary_logloss: 0.0594357\n",
      "[278]\tvalid_0's binary_logloss: 0.059359\n",
      "[279]\tvalid_0's binary_logloss: 0.0593264\n",
      "[280]\tvalid_0's binary_logloss: 0.0593017\n",
      "[281]\tvalid_0's binary_logloss: 0.0592836\n",
      "[282]\tvalid_0's binary_logloss: 0.0592528\n",
      "[283]\tvalid_0's binary_logloss: 0.0591511\n",
      "[284]\tvalid_0's binary_logloss: 0.0591205\n",
      "[285]\tvalid_0's binary_logloss: 0.059071\n",
      "[286]\tvalid_0's binary_logloss: 0.059049\n",
      "[287]\tvalid_0's binary_logloss: 0.0590353\n",
      "[288]\tvalid_0's binary_logloss: 0.0590191\n",
      "[289]\tvalid_0's binary_logloss: 0.0589842\n",
      "[290]\tvalid_0's binary_logloss: 0.0589744\n",
      "[291]\tvalid_0's binary_logloss: 0.0589556\n",
      "[292]\tvalid_0's binary_logloss: 0.0588934\n",
      "[293]\tvalid_0's binary_logloss: 0.0588906\n",
      "[294]\tvalid_0's binary_logloss: 0.0588803\n",
      "[295]\tvalid_0's binary_logloss: 0.0588375\n",
      "[296]\tvalid_0's binary_logloss: 0.0588404\n",
      "[297]\tvalid_0's binary_logloss: 0.0588785\n",
      "[298]\tvalid_0's binary_logloss: 0.0588472\n",
      "[299]\tvalid_0's binary_logloss: 0.0588308\n",
      "[300]\tvalid_0's binary_logloss: 0.058769\n",
      "[301]\tvalid_0's binary_logloss: 0.0587418\n",
      "[302]\tvalid_0's binary_logloss: 0.0587527\n",
      "[303]\tvalid_0's binary_logloss: 0.0587192\n",
      "[304]\tvalid_0's binary_logloss: 0.0586573\n",
      "[305]\tvalid_0's binary_logloss: 0.0586404\n",
      "[306]\tvalid_0's binary_logloss: 0.058622\n",
      "[307]\tvalid_0's binary_logloss: 0.0586334\n",
      "[308]\tvalid_0's binary_logloss: 0.0586364\n",
      "[309]\tvalid_0's binary_logloss: 0.0585544\n",
      "[310]\tvalid_0's binary_logloss: 0.0585372\n",
      "[311]\tvalid_0's binary_logloss: 0.0584978\n",
      "[312]\tvalid_0's binary_logloss: 0.0584987\n",
      "[313]\tvalid_0's binary_logloss: 0.0585208\n",
      "[314]\tvalid_0's binary_logloss: 0.0584339\n",
      "[315]\tvalid_0's binary_logloss: 0.0584805\n",
      "[316]\tvalid_0's binary_logloss: 0.0584598\n",
      "[317]\tvalid_0's binary_logloss: 0.0584194\n",
      "[318]\tvalid_0's binary_logloss: 0.0584351\n",
      "[319]\tvalid_0's binary_logloss: 0.0583617\n",
      "[320]\tvalid_0's binary_logloss: 0.0584063\n",
      "[321]\tvalid_0's binary_logloss: 0.0583674\n",
      "[322]\tvalid_0's binary_logloss: 0.058335\n",
      "[323]\tvalid_0's binary_logloss: 0.0583222\n",
      "[324]\tvalid_0's binary_logloss: 0.0583242\n",
      "[325]\tvalid_0's binary_logloss: 0.0583208\n",
      "[326]\tvalid_0's binary_logloss: 0.0583072\n",
      "[327]\tvalid_0's binary_logloss: 0.0583087\n",
      "[328]\tvalid_0's binary_logloss: 0.0582478\n",
      "[329]\tvalid_0's binary_logloss: 0.0582042\n",
      "[330]\tvalid_0's binary_logloss: 0.0582145\n",
      "[331]\tvalid_0's binary_logloss: 0.0581808\n",
      "[332]\tvalid_0's binary_logloss: 0.0581336\n",
      "[333]\tvalid_0's binary_logloss: 0.0580928\n",
      "[334]\tvalid_0's binary_logloss: 0.0580849\n",
      "[335]\tvalid_0's binary_logloss: 0.0580796\n",
      "[336]\tvalid_0's binary_logloss: 0.0580674\n",
      "[337]\tvalid_0's binary_logloss: 0.0580251\n",
      "[338]\tvalid_0's binary_logloss: 0.0580235\n",
      "[339]\tvalid_0's binary_logloss: 0.0580452\n",
      "[340]\tvalid_0's binary_logloss: 0.0580438\n",
      "[341]\tvalid_0's binary_logloss: 0.0580183\n",
      "[342]\tvalid_0's binary_logloss: 0.0579744\n",
      "[343]\tvalid_0's binary_logloss: 0.0579278\n",
      "[344]\tvalid_0's binary_logloss: 0.0579624\n",
      "[345]\tvalid_0's binary_logloss: 0.0579183\n",
      "[346]\tvalid_0's binary_logloss: 0.0578805\n",
      "[347]\tvalid_0's binary_logloss: 0.0578546\n",
      "[348]\tvalid_0's binary_logloss: 0.0578637\n",
      "[349]\tvalid_0's binary_logloss: 0.0578481\n",
      "[350]\tvalid_0's binary_logloss: 0.0578068\n",
      "[351]\tvalid_0's binary_logloss: 0.0577742\n",
      "[352]\tvalid_0's binary_logloss: 0.0577607\n",
      "[353]\tvalid_0's binary_logloss: 0.0577717\n",
      "[354]\tvalid_0's binary_logloss: 0.0577465\n",
      "[355]\tvalid_0's binary_logloss: 0.057683\n",
      "[356]\tvalid_0's binary_logloss: 0.0576831\n",
      "[357]\tvalid_0's binary_logloss: 0.0576545\n",
      "[358]\tvalid_0's binary_logloss: 0.0576088\n",
      "[359]\tvalid_0's binary_logloss: 0.0576072\n",
      "[360]\tvalid_0's binary_logloss: 0.0576064\n",
      "[361]\tvalid_0's binary_logloss: 0.0575511\n",
      "[362]\tvalid_0's binary_logloss: 0.0575379\n",
      "[363]\tvalid_0's binary_logloss: 0.057519\n",
      "[364]\tvalid_0's binary_logloss: 0.0575139\n",
      "[365]\tvalid_0's binary_logloss: 0.0575214\n",
      "[366]\tvalid_0's binary_logloss: 0.0575231\n",
      "[367]\tvalid_0's binary_logloss: 0.0574973\n",
      "[368]\tvalid_0's binary_logloss: 0.057461\n",
      "[369]\tvalid_0's binary_logloss: 0.0574334\n",
      "[370]\tvalid_0's binary_logloss: 0.0574623\n",
      "[371]\tvalid_0's binary_logloss: 0.057493\n",
      "[372]\tvalid_0's binary_logloss: 0.0574465\n",
      "[373]\tvalid_0's binary_logloss: 0.0574339\n",
      "[374]\tvalid_0's binary_logloss: 0.057409\n",
      "[375]\tvalid_0's binary_logloss: 0.0574021\n",
      "[376]\tvalid_0's binary_logloss: 0.0574207\n",
      "[377]\tvalid_0's binary_logloss: 0.0574314\n",
      "[378]\tvalid_0's binary_logloss: 0.0574078\n",
      "[379]\tvalid_0's binary_logloss: 0.0574332\n",
      "[380]\tvalid_0's binary_logloss: 0.0574589\n",
      "[381]\tvalid_0's binary_logloss: 0.0574343\n",
      "[382]\tvalid_0's binary_logloss: 0.0574476\n",
      "[383]\tvalid_0's binary_logloss: 0.0574352\n",
      "[384]\tvalid_0's binary_logloss: 0.0574411\n",
      "[385]\tvalid_0's binary_logloss: 0.0573987\n",
      "[386]\tvalid_0's binary_logloss: 0.05745\n",
      "[387]\tvalid_0's binary_logloss: 0.0574521\n",
      "[388]\tvalid_0's binary_logloss: 0.0574326\n",
      "[389]\tvalid_0's binary_logloss: 0.0574658\n",
      "[390]\tvalid_0's binary_logloss: 0.0574653\n",
      "[391]\tvalid_0's binary_logloss: 0.0574554\n",
      "[392]\tvalid_0's binary_logloss: 0.0573932\n",
      "[393]\tvalid_0's binary_logloss: 0.057419\n",
      "[394]\tvalid_0's binary_logloss: 0.0573903\n",
      "[395]\tvalid_0's binary_logloss: 0.0573864\n",
      "[396]\tvalid_0's binary_logloss: 0.0573737\n",
      "[397]\tvalid_0's binary_logloss: 0.0573599\n",
      "[398]\tvalid_0's binary_logloss: 0.0573096\n",
      "[399]\tvalid_0's binary_logloss: 0.0572998\n",
      "[400]\tvalid_0's binary_logloss: 0.0573153\n",
      "[401]\tvalid_0's binary_logloss: 0.057298\n",
      "[402]\tvalid_0's binary_logloss: 0.0572952\n",
      "[403]\tvalid_0's binary_logloss: 0.0573328\n",
      "[404]\tvalid_0's binary_logloss: 0.0573195\n",
      "[405]\tvalid_0's binary_logloss: 0.0572559\n",
      "[406]\tvalid_0's binary_logloss: 0.0572395\n",
      "[407]\tvalid_0's binary_logloss: 0.0572513\n",
      "[408]\tvalid_0's binary_logloss: 0.0572706\n",
      "[409]\tvalid_0's binary_logloss: 0.0572325\n",
      "[410]\tvalid_0's binary_logloss: 0.0572066\n",
      "[411]\tvalid_0's binary_logloss: 0.0571746\n",
      "[412]\tvalid_0's binary_logloss: 0.0571465\n",
      "[413]\tvalid_0's binary_logloss: 0.0571219\n",
      "[414]\tvalid_0's binary_logloss: 0.0571277\n",
      "[415]\tvalid_0's binary_logloss: 0.0571202\n",
      "[416]\tvalid_0's binary_logloss: 0.057103\n",
      "[417]\tvalid_0's binary_logloss: 0.0571017\n",
      "[418]\tvalid_0's binary_logloss: 0.0570842\n",
      "[419]\tvalid_0's binary_logloss: 0.0571617\n",
      "[420]\tvalid_0's binary_logloss: 0.0571864\n",
      "[421]\tvalid_0's binary_logloss: 0.0571879\n",
      "[422]\tvalid_0's binary_logloss: 0.0571472\n",
      "[423]\tvalid_0's binary_logloss: 0.0571489\n",
      "[424]\tvalid_0's binary_logloss: 0.0571574\n",
      "[425]\tvalid_0's binary_logloss: 0.0571516\n",
      "[426]\tvalid_0's binary_logloss: 0.0571642\n",
      "[427]\tvalid_0's binary_logloss: 0.0571487\n",
      "[428]\tvalid_0's binary_logloss: 0.0571491\n",
      "Early stopping, best iteration is:\n",
      "[418]\tvalid_0's binary_logloss: 0.0570842\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "0.5032829681196108\n",
      "The acc of prediction is 0.975476623684835\n",
      "The precision of prediction is 0.9757937755422823\n",
      "The recall of prediction is 0.9754871150219987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "The acc of all prediction is 0.9950952471362572\n",
      "The precision of all prediction is 0.995126582278481\n",
      "The recall of all prediction is 0.9950636035693944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import zipfile\n",
    "import os \n",
    "import py7zr\n",
    "import multiprocessing \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error, classification_report\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import random\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "#6和3；7和3；8和3\n",
    "DRtg_bins = 8\n",
    "PM_bins = 3\n",
    "DRtg_Fuzzification = True\n",
    "# DRtg_Fuzzification = False\n",
    "PM_Fuzzification = True\n",
    "# PM_Fuzzification = False\n",
    "\n",
    "def data_load(data):\n",
    "    df = data\n",
    "    # print(df.info())\n",
    "    # print(df.columns)\n",
    "    # 将时间转换为浮点数\n",
    "    time_features = []\n",
    "    for i in range(36):\n",
    "        feature = 'MP' + '_' + str(i)\n",
    "        time_features.append(feature)\n",
    "    # print(time_features) \n",
    "    \n",
    "    df = df[~df['BPM_0'].astype(str).str.contains(':')]\n",
    "    # print(df['BPM_0'].astype(str).str.contains(':'))\n",
    "    print('清洗后数据：',len(df))\n",
    "    df[time_features] = df[time_features].fillna('0:0')\n",
    "    df[time_features] = df[time_features].map(lambda x: '0:0' if ':' not in str(x) else x)\n",
    "    df[time_features] =df[time_features].map(lambda x: float(x.replace(':', '.')))\n",
    "    df = df.fillna(0)\n",
    "    if  DRtg_Fuzzification:\n",
    "        #  #缩小'ORtg','DRtg'影响\n",
    "        norm_features = []\n",
    "        for i in range(36):\n",
    "            for feat in ['DRtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "        # df = df.fillna(0)\n",
    "        # # 转换为数值类型\n",
    "        df[norm_features] = df[norm_features].apply(pd.to_numeric, errors='coerce')\n",
    "        # 将负值变为0\n",
    "        df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n",
    "        # 将 norm_features 值聚合在一起\n",
    "        norm_feature_values = df[norm_features].values.flatten().reshape(-1, 1)    \n",
    "        print(norm_feature_values.shape)\n",
    "        # 将数据划分为 10 个区间进行数值离散化处理\n",
    "        kdb = KBinsDiscretizer(n_bins = DRtg_bins, encode = 'ordinal', strategy='quantile')\n",
    "        kdb.fit(norm_feature_values)\n",
    "        for norm_feature in norm_features:\n",
    "            df[norm_feature] = kdb.transform(df[norm_feature].values.reshape(-1, 1))\n",
    "    if  PM_Fuzzification:\n",
    "        norm_features = []\n",
    "        for i in range(36):\n",
    "            for feat in ['+/-']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "        # # 转换为数值类型\n",
    "        df[norm_features] = df[norm_features].apply(pd.to_numeric, errors='coerce')\n",
    "        # 将负值变为0\n",
    "        df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n",
    "        # 将 norm_features 值聚合在一起\n",
    "        norm_feature_values = df[norm_features].values.flatten().reshape(-1, 1)    \n",
    "        print(norm_feature_values.shape)\n",
    "        # 将数据划分为 10 个区间进行数值离散化处理\n",
    "        kdb = KBinsDiscretizer(n_bins = PM_bins, encode = 'ordinal', strategy='quantile')\n",
    "        kdb.fit(norm_feature_values)\n",
    "        for norm_feature in norm_features:\n",
    "            df[norm_feature] = kdb.transform(df[norm_feature].values.reshape(-1, 1))\n",
    "    print(df[['DRtg_0','DRtg_35']])\n",
    "    print(df[['+/-_0','+/-_35']])\n",
    "    print(df.info())\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "def data_prerpocess(df, training = True):\n",
    "    \n",
    "    #V_0：全有\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_1去除+-，DRtg\n",
    "    used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'BPM']\n",
    "    #V_2去除MP,+-，DRtg\n",
    "    # used_features = ['FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','BPM']\n",
    "    #v_3 +-,'BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','DRtg']\n",
    "    # V_4去除PTS,+-,'ORtg','BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_5 去除+-,'ORtg','DRtg', 'BPM'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_6 去除+-,'ORtg','DRtg', 'BPM','PTS'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_7 缩小+-,'BPM','ORtg','DRtg', \n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_8 去除+-,'BPM','ORtg',通过模糊化降低'DRtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_9 去除+-,'BPM',通过模糊化降低'DRtg','ORtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg','ORtg']\n",
    "    #v_10 去除+-,通过模糊化降低'DRtg','ORtg'影响值\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%','ORtg', 'DRtg', 'BPM']\n",
    "    #v_11 模糊化+-,'DRtg'\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    total_over_features = []\n",
    "    for i in range(36):\n",
    "        for feature in used_features:\n",
    "            feature = feature + '_' + str(i)\n",
    "            total_over_features.append(feature)\n",
    "    \n",
    "    print('选择的特征数量：',len(total_over_features))\n",
    "    X = df[total_over_features].astype(float)\n",
    "    Y = df['result'].astype(int)\n",
    "    X_list.append(X)\n",
    "    Y_list.append(Y)\n",
    "    # print('处理后数据：',len(X))\n",
    " \n",
    "    # #打乱同一个队球员顺序添加训练数据进去\n",
    "    # random.seed(0)\n",
    "    # for sample in range(15):\n",
    "    #     total_over_features = []\n",
    "    #     for i in random.sample(range(18), 18):  # 打乱顺序\n",
    "    #         for feature in used_features:\n",
    "    #             new_feature = feature + '_' + str(i)\n",
    "    #             total_over_features.append(new_feature)\n",
    "                \n",
    "    #     for i in random.sample(range(18,36), 18):  # 打乱顺序\n",
    "    #         for feature in used_features:\n",
    "    #             new_feature = feature + '_' + str(i)\n",
    "    #             total_over_features.append(new_feature)\n",
    "                \n",
    "    #     print('特征顺序：',total_over_features)\n",
    "    #     X1 = df[total_over_features].astype(float)\n",
    "    #     Y1 = df['result'].astype(int)\n",
    "    #     X_list.append(X1)\n",
    "    #     Y_list.append(Y1)\n",
    "        \n",
    "    X = pd.concat(X_list, ignore_index=True)\n",
    "    Y = pd.concat(Y_list, ignore_index=True)    \n",
    "    print('处理后数据：',len(X))\n",
    "    \n",
    "    if training:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) \n",
    "        return X_train[total_over_features], X_test[total_over_features], Y_train, Y_test\n",
    "    else:\n",
    "        return X[total_over_features], Y, total_over_features\n",
    "\n",
    "def lgb_train(X_train, X_test, y_train, y_test):\n",
    "    norm_features = []\n",
    "    if  DRtg_Fuzzification:\n",
    "        for i in range(36):\n",
    "            for feat in ['DRtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "    if  PM_Fuzzification:\n",
    "        for i in range(36):\n",
    "            for feat in ['+/-']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "    categorical_feature = norm_features\n",
    "    print(categorical_feature)\n",
    "    model = lightgbm.LGBMClassifier(objective='binary', learning_rate=0.1, num_leaves = 30, min_data_in_leaf = 100, feature_fraction = 0.9, max_depth = 8, n_estimators=1500)\n",
    "    callbacks = [log_evaluation(period=1), early_stopping(stopping_rounds=10)]\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='binary_logloss', callbacks = callbacks, categorical_feature = categorical_feature)\n",
    "    joblib.dump(model, 'model/nba_mvp_lgb_v12.pkl')\n",
    "    model = joblib.load('model/nba_mvp_lgb_v12.pkl')\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "\n",
    "\n",
    "    print(np.average(y_pred))\n",
    "    print('The acc of prediction is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "    print('The precision of prediction is {}'.format(precision_score(y_test, y_pred)))\n",
    "    print('The recall of prediction is {}'.format(recall_score(y_test, y_pred)))\n",
    "\n",
    "    X = pd.concat((X_train, X_test))\n",
    "    y = np.append(y_train, y_test)\n",
    "    y_all_pred = model.predict(X, num_iteration=model.best_iteration_)\n",
    "    print('The acc of all prediction is {}'.format(accuracy_score(y, y_all_pred)))\n",
    "    print('The precision of all prediction is {}'.format(precision_score(y, y_all_pred)))\n",
    "    print('The recall of all prediction is {}'.format(recall_score(y, y_all_pred)))\n",
    "\n",
    "def rawdata_load(extract_folder):\n",
    "    file_path_list = []\n",
    "    df_list = []\n",
    "    # 遍历解压后的文件夹，并读取每个 CSV 文件\n",
    "    for root, dirs, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\"basic_stats.csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_path_list.append(file_path)\n",
    "                # print(file_path)\n",
    "                # 使用 pandas 读取 CSV 文件\n",
    "                df_basic = pd.read_csv(file_path).drop_duplicates()\n",
    "                df_advanced = pd.read_csv(file_path.replace('basic_stats.csv','advanced_stats.csv')).drop_duplicates()\n",
    "                if df_basic['PTS'].dtype=='object':\n",
    "                    df_basic = df_basic.drop(df_basic.index[-1])\n",
    "                    df_advanced = df_advanced.drop(df_advanced.index[-1])\n",
    "                # 连接两个文件：63162\n",
    "                df = pd.concat([df_basic, df_advanced], axis=1)\n",
    "                df = df.loc[:,~df.columns.duplicated()]\n",
    "                # 将文件名作为新列添加到 DataFrame\n",
    "                df['file_name'] = file_path.split('/')[-1]\n",
    "                df_list.append(df)\n",
    "    print(len(file_path_list),len(df_list)) \n",
    "    max_player = 18\n",
    "\n",
    "    features = ['starters', 'team', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "        'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "        'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM',\n",
    "        'file_name']\n",
    "    # 连接所有球员数据\n",
    "    new_features = []\n",
    "    for i in range(max_player*2):\n",
    "        for feat in features:\n",
    "            new_feat = feat + '_' + str(i)\n",
    "            new_features.append(new_feat)\n",
    "    # print(len(features), len(new_features))\n",
    "\n",
    "    result_features = ['score_ourside', 'score_opposite', 'result']\n",
    "\n",
    "    new_data = pd.DataFrame(columns = new_features + result_features)\n",
    "    for i in tqdm(range(len(file_path_list))):\n",
    "        # for i in range(10):\n",
    "        path = file_path_list[i]\n",
    "        # print(path.split('/')[-1])\n",
    "        df1 = df_list[i]\n",
    "        filled_df1 = df1.reindex(range(max_player))\n",
    "        #对手球队数据\n",
    "        a = path.split('/')[-1].split('_')[3]\n",
    "        b = path.split('/')[-1].split('_')[5]\n",
    "        oppo_path = path.replace(a+'_vs_'+b, b+'_vs_'+a)\n",
    "        # print(oppo_path,file_path_list.index(oppo_path))\n",
    "        df2 = df_list[file_path_list.index(oppo_path)]\n",
    "        filled_df2 = df2.reindex(range(max_player))\n",
    "        # print(filled_df1[\"PTS\"],filled_df2[\"PTS\"])\n",
    "        score_ourside = filled_df1[\"PTS\"].astype(float).sum()\n",
    "        score_opposite = filled_df2[\"PTS\"].astype(float).sum()\n",
    "        if score_ourside > score_opposite:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "        \n",
    "        feature_data1 = []\n",
    "        feature_data2 = []\n",
    "        for num in range(max_player):\n",
    "            feature_data1 += filled_df1.loc[num, features].tolist() \n",
    "            feature_data2 += filled_df2.loc[num, features].tolist() \n",
    "            \n",
    "        new_data.loc[i, new_features + result_features] = feature_data1 + feature_data2 + [score_ourside, score_opposite, result]\n",
    "    return new_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = './data/nba_mvp.csv'\n",
    "    df1 = pd.read_csv(data_path)\n",
    "    #加上24年数据\n",
    "    folder_path = 'data/basketball_data_info/'\n",
    "    df2 = rawdata_load(folder_path)\n",
    "    df = pd.concat([df1, df2], axis=0)  \n",
    "    print(len(df1),len(df2),len(df))\n",
    "    data = data_load(df)\n",
    "    X_train, X_test, y_train, y_test = data_prerpocess(data)\n",
    "    lgb_train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022Finals比赛数据： 12\n",
      "选择的特征数量： 1260\n",
      "处理后数据： 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvp_shap/lib/python3.9/site-packages/shap/explainers/_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 5596.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1260) (12, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 15.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# 24,22\n",
    "season = 'Finals/'\n",
    "model_path = 'model/nba_mvp_lgb_v12.pkl'\n",
    "if year == 2024:\n",
    "    # folder_path = 'data/basketball_data_info/'\n",
    "    day1 ='2024_06_06'\n",
    "    day2 ='2024_06_09'\n",
    "    day3 ='2024_06_12'\n",
    "    day4 ='2024_06_14'\n",
    "    day5 ='2024_06_17'\n",
    "    # folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2024_06_06|2024_06_09|2024_06_12|2024_06_14|2024_06_17'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2023:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2023_06_01'\n",
    "    day2 ='2023_06_04'\n",
    "    day3 ='2023_06_07'\n",
    "    day4 ='2023_06_09'\n",
    "    day5 ='2023_06_12'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2023_06_01|2023_06_04|2023_06_07|2023_06_09|2023_06_12'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2022:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2022_06_02'\n",
    "    day2 ='2022_06_05'\n",
    "    day3 ='2022_06_08'\n",
    "    day4 ='2022_06_10'\n",
    "    day5 ='2022_06_13'\n",
    "    day6 ='2022_06_16'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2022_06_02|2022_06_05|2022_06_08|2022_06_10|2022_06_13|2022_06_16'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2021:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2021_07_06'\n",
    "    day2 ='2021_07_08'\n",
    "    day3 ='2021_07_11'\n",
    "    day4 ='2021_07_14'\n",
    "    day5 ='2021_07_17'\n",
    "    day6 ='2021_07_20'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2021_07_06|2021_07_08|2021_07_11|2021_07_14|2021_07_17|2021_07_20'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2020:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2020_09_30'\n",
    "    day2 ='2020_10_02'\n",
    "    day3 ='2020_10_04'\n",
    "    day4 ='2020_10_06'\n",
    "    day5 ='2020_10_09'\n",
    "    day6 ='2020_10_11'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5,folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2020_09_30|2020_10_02|2020_10_04|2020_10_06|2020_10_09|2020_10_11'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2019:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2019_05_30'\n",
    "    day2 ='2019_06_02'\n",
    "    day3 ='2019_06_05'\n",
    "    day4 ='2019_06_07'\n",
    "    day5 ='2019_06_10'\n",
    "    day6 ='2019_06_13'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5,folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2019_05_30|2019_06_02|2019_06_05|2019_06_07|2019_06_10|2019_06_13'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2018:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2018_05_31'\n",
    "    day2 ='2018_06_03'\n",
    "    day3 ='2018_06_06'\n",
    "    day4 ='2018_06_08'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2018_05_31|2018_06_03|2018_06_06|2018_06_08'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2017:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2017_06_01'\n",
    "    day2 ='2017_06_04'\n",
    "    day3 ='2017_06_07'\n",
    "    day4 ='2017_06_09'\n",
    "    day5 ='2017_06_12'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2017_06_01|2017_06_04|2017_06_07|2017_06_09|2017_06_12'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2016:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2016_06_02'\n",
    "    day2 ='2016_06_05'\n",
    "    day3 ='2016_06_08'\n",
    "    day4 ='2016_06_10'\n",
    "    day5 ='2016_06_13'\n",
    "    day6 ='2016_06_16'\n",
    "    day7 ='2016_06_19'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6, folder_path+day7]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2016_06_02|2016_06_05|2016_06_08|2016_06_10|2016_06_13|2016_06_16|2016_06_19'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2015:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2015_06_04'\n",
    "    day2 ='2015_06_07'\n",
    "    day3 ='2015_06_09'\n",
    "    day4 ='2015_06_11'\n",
    "    day5 ='2015_06_14'\n",
    "    day6 ='2015_06_16'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2015_06_04|2015_06_07|2015_06_09|2015_06_11|2015_06_14|2015_06_16'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2014:\n",
    "    folder_path = 'data/nba/basketball_reference_part2_finish/'\n",
    "    day1 ='2014_06_05'\n",
    "    day2 ='2014_06_08'\n",
    "    day3 ='2014_06_10'\n",
    "    day4 ='2014_06_12'\n",
    "    day5 ='2014_06_15'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2014_06_05|2014_06_08|2014_06_10|2014_06_12|2014_06_15'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2013:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2013_06_06'\n",
    "    day2 ='2013_06_09'\n",
    "    day3 ='2013_06_11'\n",
    "    day4 ='2013_06_13'\n",
    "    day5 ='2013_06_16'\n",
    "    day6 ='2013_06_18'\n",
    "    day7 ='2013_06_20'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6, folder_path+day7]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2013_06_06|2013_06_09|2013_06_11|2013_06_13|2013_06_16|2013_06_18|2013_06_20'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2012:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2012_06_12'\n",
    "    day2 ='2012_06_14'\n",
    "    day3 ='2012_06_17'\n",
    "    day4 ='2012_06_19'\n",
    "    day5 ='2012_06_21'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2012_06_12|2012_06_14|2012_06_17|2012_06_19|2012_06_21'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2011:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2011_05_31'\n",
    "    day2 ='2011_06_02'\n",
    "    day3 ='2011_06_05'\n",
    "    day4 ='2011_06_07'\n",
    "    day5 ='2011_06_09'\n",
    "    day6 ='2011_06_12'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2011_05_31|2011_06_02|2011_06_05|2011_06_07|2011_06_09|2011_06_12'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2010:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2010_06_03'\n",
    "    day2 ='2010_06_06'\n",
    "    day3 ='2010_06_08'\n",
    "    day4 ='2010_06_10'\n",
    "    day5 ='2010_06_13'\n",
    "    day6 ='2010_06_15'\n",
    "    day7 ='2010_06_17'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6, folder_path+day7]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2010_06_03|2010_06_06|2010_06_08|2010_06_10|2010_06_13|2010_06_15|2010_06_17'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2009:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2009_06_04'\n",
    "    day2 ='2009_06_07'\n",
    "    day3 ='2009_06_09'\n",
    "    day4 ='2009_06_11'\n",
    "    day5 ='2009_06_14'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2009_06_04|2009_06_07|2009_06_09|2009_06_11|2009_06_14'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2008:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2008_06_05'\n",
    "    day2 ='2008_06_08'\n",
    "    day3 ='2008_06_10'\n",
    "    day4 ='2008_06_12'\n",
    "    day5 ='2008_06_15'\n",
    "    day6 ='2008_06_17'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2008_06_05|2008_06_08|2008_06_10|2008_06_12|2008_06_15|2008_06_17'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2007:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2007_06_07'\n",
    "    day2 ='2007_06_10'\n",
    "    day3 ='2007_06_12'\n",
    "    day4 ='2007_06_14'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2007_06_07|2007_06_10|2007_06_12|2007_06_14'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "if year == 2006:\n",
    "    folder_path = 'data/nba/basketball_reference/'\n",
    "    day1 ='2006_06_08'\n",
    "    day2 ='2006_06_11'\n",
    "    day3 ='2006_06_13'\n",
    "    day4 ='2006_06_15'\n",
    "    day5 ='2006_06_18'\n",
    "    day6 ='2006_06_20'\n",
    "    folder_list = [folder_path+day1,folder_path+day2,folder_path+day3, folder_path+day4, folder_path+day5, folder_path+day6]\n",
    "    final_data = data[data.apply(lambda x: x.astype(str).str.contains('2006_06_08|2006_06_11|2006_06_13|2006_06_15|2006_06_18|2006_06_20'\n",
    "                            , regex=True)).any(axis=1)]\n",
    "\n",
    "\n",
    "def compute_shap_values(shap_values_path, X):\n",
    "    shap_values = np.load(shap_values_path)\n",
    "    feat_num = len(shap_values[0]) // 36 #每个球员有33个特征\n",
    "    mvp_values = []\n",
    "    X = X.fillna(0)\n",
    "    for i in tqdm(range(len(X))):\n",
    "        shap_value = shap_values[i]\n",
    "        mvp_value = 0\n",
    "        for j in range(len(shap_value)):\n",
    "            mvp_value += shap_value[j]#*X.loc[i,total_over_features[j]]\n",
    "            if j % feat_num == feat_num - 1:\n",
    "                mvp_values.append(mvp_value)\n",
    "                mvp_value = 0\n",
    "    abs_array = np.abs(np.array(mvp_values).reshape(-1,36))\n",
    "    # print('1111111111111:',mvp_values,X)\n",
    "    sum_first_three = np.sum(abs_array[:, :18], axis=1, keepdims=True)\n",
    "    x = 0.5\n",
    "    normalized_first_three = (abs_array[:, :18] / sum_first_three) * x\n",
    "\n",
    "    # 后三列数值除以后三列的和并乘以系数（1-x）\n",
    "    sum_last_three = np.sum(abs_array[:, 18:], axis=1, keepdims=True)\n",
    "    normalized_last_three = (abs_array[:, 18:] / sum_last_three) * (1 - x)\n",
    "\n",
    "    # 合并处理后的数组\n",
    "    result_array = np.hstack((normalized_first_three, normalized_last_three))\n",
    "    return result_array, np.array(mvp_values).reshape(-1,36), shap_values\n",
    "\n",
    "def explain(X, model_path, shap_file_name):\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    # print(explainer.expected_value)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    \n",
    "    np.save(shap_file_name, shap_values)\n",
    "\n",
    "    shap_values = np.load(shap_file_name)\n",
    "    print(shap_values.shape)\n",
    "   \n",
    "# 队内排名\n",
    "def rank(values, order = 1):\n",
    "    if order == 1:\n",
    "        sorted_indices = np.argsort(values)[::-1]\n",
    "    else:\n",
    "        sorted_indices = np.argsort(values)\n",
    "    ranks = np.empty_like(sorted_indices)\n",
    "    ranks[sorted_indices] = np.arange(1, len(sorted_indices) + 1)\n",
    "\n",
    "    return ranks, sorted_indices[0]\n",
    "\n",
    "def process_data(df, feat_values, shap_values):\n",
    "    df, feat_values, shap_values\n",
    "    #V_0\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    # #v_1去除+-，DRtg\n",
    "    used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'BPM']\n",
    "    #V_2\n",
    "    # used_features = ['FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','BPM']\n",
    "    #v_3\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','DRtg']\n",
    "    # V_4\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_5\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    # v_6 去除+-,'ORtg','DRtg', 'BPM','PTS'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_7 缩小+-,'BPM','ORtg','DRtg', \n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_8 去除+-,'BPM','ORtg',通过模糊化降低'DRtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_9 去除+-,'BPM',通过模糊化降低'DRtg','ORtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg','ORtg']\n",
    "    #v_10 去除+-,通过模糊化降低'DRtg','ORtg'影响值\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%','ORtg', 'DRtg', 'BPM']\n",
    "    #v_11 去除+-,'DRtg',打乱球员位置\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%','ORtg', 'BPM']\n",
    "    #v_12 模糊化+-,'DRtg'\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    \n",
    "    feature_value = []\n",
    "    id_feature = 'starters'\n",
    "    id_features = [id_feature + '_' + str(i) for i in range(36)]\n",
    "    \n",
    "\n",
    "    score_features = ['score_ourside', 'score_opposite']\n",
    "    total_over_features = []\n",
    "    feat_num = len(used_features)\n",
    "    \n",
    "    for feature in used_features:\n",
    "        feature_value.append(feature)\n",
    "        feature_value.append(feature + '_shap_value')\n",
    "    value_name = [feature_value[i] for i in range(1, len(feature_value), 2)]\n",
    "\n",
    "    for i in range(36):\n",
    "        for feature in used_features:\n",
    "            feature = feature + '_' + str(i)\n",
    "            total_over_features.append(feature)\n",
    "  \n",
    "    #ds, role_id\n",
    "    new_data = pd.DataFrame(columns = ['file_name', 'result', 'starters', 'home_team', 'team_score'] + feature_value + [ 'shap_score', 'shap_order_team'])\n",
    "    for i in tqdm(range(len(df))):\n",
    "        result = df.loc[i, 'result']\n",
    "        match_id = df.loc[i, 'file_name_0']\n",
    "      \n",
    "        self_shap_values = shap_values[i][:18]\n",
    "        opp_shap_values = shap_values[i][18:]\n",
    "        \n",
    "        for j in range(18):\n",
    "            #ds, role_id, feat_shap_value\n",
    "            new_data.loc[36 * i + j, ['file_name', 'result', 'starters', 'team_score'] + used_features] = df.loc[i, ['file_name_0', 'result',  id_features[j], score_features[0]] + total_over_features[j * feat_num : (j + 1) * feat_num]].tolist()\n",
    "            new_data.loc[36 * i + j, 'home_team'] = '1'\n",
    "            new_data.loc[36 * i + j, value_name] = feat_values[i][j * feat_num : (j + 1) * feat_num] #n * feat_num * 6\n",
    "            \n",
    "        rows = [36 * i + n for n in range(18)]\n",
    "        shap_ranks, shap_index1 = rank(self_shap_values)\n",
    "        new_data.loc[rows, 'shap_order_team'] = shap_ranks\n",
    "        \n",
    "        \n",
    "        for j in range(18, 36):\n",
    "            new_data.loc[36 * i + j, ['file_name',  'starters', 'team_score'] + used_features] = df.loc[i, ['file_name_18', id_features[j], score_features[1]] + total_over_features[j * feat_num : (j + 1) * feat_num]].tolist()\n",
    "            new_data.loc[36 * i + j, 'home_team'] = '0'\n",
    "            new_data.loc[36 * i + j, 'result'] = 1 - df.loc[i, 'result']\n",
    "            new_data.loc[36 * i + j, value_name] = feat_values[i][j * feat_num : (j + 1) * feat_num]\n",
    "           \n",
    "        rows = [36 * i + n for n in range(18,36)]\n",
    "        shap_ranks, shap_index2 = rank(opp_shap_values, -1)\n",
    "        new_data.loc[rows, 'shap_order_team'] = shap_ranks\n",
    "\n",
    "    new_data['shap_score'] = shap_values.reshape(-1)\n",
    "    return new_data\n",
    "\n",
    "print('{}Finals比赛数据：'.format(year),len(final_data))\n",
    "# print(data['file_name_0'])\n",
    "X, Y, total_over_features = data_prerpocess(final_data, training= False) \n",
    "shap_file_name = './shap/'+season+'{}_nba_mvp_shap_values_v12.npy'.format(year)\n",
    "# if not os.path.exists(shap_file_name):\n",
    "explain(X, model_path, shap_file_name)\n",
    "money_values, shap_values, feat_values = compute_shap_values(shap_file_name, X)\n",
    "\n",
    "\n",
    "print(feat_values.shape,shap_values.shape)\n",
    "processed_data = process_data(final_data.reset_index(drop = True), feat_values, shap_values)\n",
    "processed_data.to_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "# 绘制summary分析图并保存\n",
    "def summary_bar_plot(shap_values, max_display=20, show=False, save_path=None):\n",
    "    plt.style.use('ggplot')\n",
    "    shap.plots.bar(shap_values, max_display=max_display, show=show)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.encode(encoding='utf-8'), dpi=200, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "def summary_dot_plot(shap_values, max_display=20, show=False, save_path=None):\n",
    "    plt.style.use('ggplot')\n",
    "    shap.plots.beeswarm(shap_values, max_display=max_display, show=show)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.encode(encoding='utf-8'), dpi=200, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "# 绘制summary分析图并保存\n",
    "save_path = 'shap/'+season+'{}_nba_mvp_summary_dot_v12.png'.format(year)\n",
    "summary_dot_plot(shap.Explanation(\n",
    "values=feat_values, \n",
    "# base_values = 0,\n",
    "data=np.array(X),\n",
    "feature_names=total_over_features), max_display=50, show=False, save_path=save_path)\n",
    "\n",
    "save_path = 'shap/'+season+'{}_nba_mvp_summary_bar_v12.png'.format(year)\n",
    "summary_bar_plot(shap.Explanation(\n",
    "values=feat_values, \n",
    "# base_values = np.array(0),\n",
    "data=np.array(X),\n",
    "feature_names=total_over_features), max_display=50, show=False, save_path=save_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     1\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "5     1\n",
      "6     0\n",
      "7     1\n",
      "8     1\n",
      "9     0\n",
      "10    1\n",
      "11    0\n",
      "Name: result, dtype: int64\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[0 1 1 0 0 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# model_path = 'model/nba_mvp_lgb_v12.pkl'\n",
    "model = joblib.load(model_path)\n",
    "print(Y)\n",
    "y = model.predict(X, num_iteration=model.best_iteration_)\n",
    "# y = model.predict_proba(X, num_iteration=model.best_iteration_)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Andrew Wiggins', 'Draymond Green', 'Klay Thompson', 'Stephen Curry',\n",
      "       'Otto Porter Jr.', 'Kevon Looney', 'Jordan Poole'],\n",
      "      dtype='object', name='starters')\n",
      "7 starters\n",
      "Draymond Green     3.000\n",
      "Stephen Curry      3.375\n",
      "Andrew Wiggins     4.375\n",
      "Kevon Looney       4.750\n",
      "Klay Thompson      5.500\n",
      "Jordan Poole       6.125\n",
      "Otto Porter Jr.    9.000\n",
      "Name: shap_order_team, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#球员在所有胜场的shapley值排名平均\n",
    "data_value = pd.read_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "data_value= data_value[data_value['MP'] != 0.0]\n",
    "# print('上场人数：',len(data_value))\n",
    "result_df = \\\n",
    "data_value[data_value['starters'].\n",
    "           map(data_value['starters'].value_counts()) > len(final_data)-1]\n",
    "#取出胜方球员\n",
    "df = result_df\n",
    "result_1 = df[df['result'] == 1]\n",
    "max_result_1_count = result_1['starters'].value_counts().max()\n",
    "max_result_1_starters = result_1['starters'].value_counts()[result_1['starters'].value_counts() == max_result_1_count].index\n",
    "print(max_result_1_starters)\n",
    "result_df = df[df['starters'].isin(max_result_1_starters)]\n",
    "#胜场\n",
    "result_df_win = result_df[result_df['result'] == 1]\n",
    "mvp_by_shap = result_df_win.groupby('starters')['shap_order_team'].mean().sort_values()\n",
    "print(len(mvp_by_shap),mvp_by_shap[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 starters\n",
      "Stephen Curry      6.750000\n",
      "Draymond Green     7.833333\n",
      "Kevon Looney       7.833333\n",
      "Andrew Wiggins     8.333333\n",
      "Klay Thompson      8.916667\n",
      "Jordan Poole       9.416667\n",
      "Otto Porter Jr.    9.583333\n",
      "Name: shap_order_team, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#球员在所有场的shapley值排名平均\n",
    "mvp_by_shap = result_df.groupby('starters')['shap_order_team'].mean().sort_values()\n",
    "print(len(mvp_by_shap),mvp_by_shap[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starters\n",
      "Stephen Curry      0.915368\n",
      "Kevon Looney       0.408587\n",
      "Otto Porter Jr.    0.286604\n",
      "Andrew Wiggins     0.271679\n",
      "Draymond Green     0.243139\n",
      "Klay Thompson     -0.004156\n",
      "Jordan Poole      -0.159121\n",
      "Name: shap_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#球员在所有场的shapley值平均\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "# result_df = pd.read_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "result_df1 = result_df[result_df['home_team'] == 1]\n",
    "result_df2 = result_df[result_df['home_team'] == 0]\n",
    "\n",
    "# print(result_df['starters'].value_counts())\n",
    "mvp_by_shap1 = result_df1.groupby('starters')['shap_score'].mean()\n",
    "mvp_by_shap2 = result_df2.groupby('starters')['shap_score'].mean()\n",
    "\n",
    "mvp_by_shap = (mvp_by_shap1 - mvp_by_shap2).sort_values(ascending=False)\n",
    "print(mvp_by_shap[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starters\n",
      "Stephen Curry      1.648865\n",
      "Draymond Green     1.568513\n",
      "Andrew Wiggins     1.338613\n",
      "Kevon Looney       1.202349\n",
      "Klay Thompson      1.044415\n",
      "Jordan Poole       0.861054\n",
      "Otto Porter Jr.    0.589318\n",
      "Name: shap_score, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2331208/2411647944.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
      "/tmp/ipykernel_2331208/2411647944.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
      "/tmp/ipykernel_2331208/2411647944.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
      "/tmp/ipykernel_2331208/2411647944.py:9: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n",
      "/tmp/ipykernel_2331208/2411647944.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n",
      "/tmp/ipykernel_2331208/2411647944.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n"
     ]
    }
   ],
   "source": [
    "#输球负贡献置零\n",
    "# result_df = pd.read_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "result_df1 = result_df[result_df['home_team'] == 1]\n",
    "result_df2 = result_df[result_df['home_team'] == 0]\n",
    "#负场\n",
    "result_df_loss1 = result_df1[result_df1['result'] == 0]\n",
    "result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
    "result_df_loss2 = result_df2[result_df2['result'] == 0]\n",
    "result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n",
    "result_df1 = pd.concat([result_df_loss1, result_df1[result_df1['result'] == 1]])\n",
    "result_df2 = pd.concat([result_df_loss2, result_df2[result_df2['result'] == 1]])\n",
    "# print(result_df['starters'].value_counts())\n",
    "mvp_by_shap1 = result_df1.groupby('starters')['shap_score'].mean()\n",
    "mvp_by_shap2 = result_df2.groupby('starters')['shap_score'].mean()\n",
    "\n",
    "mvp_by_shap = (mvp_by_shap1 - mvp_by_shap2).sort_values(ascending=False)\n",
    "print(mvp_by_shap[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
