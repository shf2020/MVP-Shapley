{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import zipfile\n",
    "import os \n",
    "import py7zr\n",
    "import multiprocessing \n",
    "from tqdm import tqdm \n",
    "from concurrent.futures import ThreadPoolExecutor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设压缩文件名为 data.zip\n",
    "zip_file_path1 = 'data/basketball_reference.7z'\n",
    "zip_file_path2 = 'data/basketball_reference_part2_finish.7z'\n",
    "\n",
    "# 解压缩文件到指定目录\n",
    "extract_folder = 'data/nba'\n",
    "# 如果不存在\n",
    "if not os.path.exists(extract_folder):\n",
    "    with py7zr.SevenZipFile(zip_file_path1, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder)\n",
    "    with py7zr.SevenZipFile(zip_file_path2, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder)\n",
    "        \n",
    "file_path_list = []\n",
    "df_list = []\n",
    "# 遍历解压后的文件夹，并读取每个 CSV 文件\n",
    "for root, dirs, files in os.walk(extract_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\"basic_stats.csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_path_list.append(file_path)\n",
    "            print(file_path)\n",
    "            # 使用 pandas 读取 CSV 文件\n",
    "            df_basic = pd.read_csv(file_path).drop_duplicates()\n",
    "            df_advanced = pd.read_csv(file_path.replace('basic_stats.csv','advanced_stats.csv')).drop_duplicates()\n",
    "            if df_basic['PTS'].dtype=='object':\n",
    "                df_basic = df_basic.drop(df_basic.index[-1])\n",
    "                df_advanced = df_advanced.drop(df_advanced.index[-1])\n",
    "            # 连接两个文件：63162\n",
    "            df = pd.concat([df_basic, df_advanced], axis=1)\n",
    "            df = df.loc[:,~df.columns.duplicated()]\n",
    "            # 将文件名作为新列添加到 DataFrame\n",
    "            df['file_name'] = file_path.split('/')[-1]\n",
    "            df_list.append(df)\n",
    "\n",
    "print(len(file_path_list),len(df_list))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 返回所有最大值的索引\n",
    "# len_df = [len(df_list[i]) for i in range(len(df_list))]\n",
    "# max_player = max(len_df)\n",
    "# max_indices = [i for i, v in enumerate(len_df) if v == max_player]\n",
    "# print(max_player, max_indices)\n",
    "# print(df_list[max_indices[0]]['starters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(file_path_list[0].split('/')[-1])\n",
    "len_df = [len(df_list[i]) for i in range(len(df_list))]\n",
    "max_player = max(len_df)\n",
    "print('max_player:',max_player)\n",
    "# 打印所有列名\n",
    "# print(df_list[0].columns)\n",
    "\n",
    "features = ['starters', 'team', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "       'PTS', '+/-', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM',\n",
    "       'file_name']\n",
    "# 连接所有球员数据\n",
    "new_features = []\n",
    "for i in range(max_player*2):\n",
    "    for feat in features:\n",
    "        new_feat = feat + '_' + str(i)\n",
    "        new_features.append(new_feat)\n",
    "print(len(features), len(new_features))\n",
    "\n",
    "result_features = ['score_ourside', 'score_opposite', 'result']\n",
    "new_data = pd.DataFrame(columns = new_features + result_features)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(file_path_list))):\n",
    "# for i in range(10):\n",
    "    path = file_path_list[i]\n",
    "    # print(path.split('/')[-1])\n",
    "    df1 = df_list[i]\n",
    "    filled_df1 = df1.reindex(range(max_player))\n",
    "    #对手球队数据\n",
    "    a = path.split('/')[-1].split('_')[3]\n",
    "    b = path.split('/')[-1].split('_')[5]\n",
    "    oppo_path = path.replace(a+'_vs_'+b, b+'_vs_'+a)\n",
    "    # print(oppo_path,file_path_list.index(oppo_path))\n",
    "    df2 = df_list[file_path_list.index(oppo_path)]\n",
    "    filled_df2 = df2.reindex(range(max_player))\n",
    "    # print(filled_df1[\"PTS\"],filled_df2[\"PTS\"])\n",
    "    score_ourside = filled_df1[\"PTS\"].astype(float).sum()\n",
    "    score_opposite = filled_df2[\"PTS\"].astype(float).sum()\n",
    "    if score_ourside > score_opposite:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    \n",
    "    feature_data1 = []\n",
    "    feature_data2 = []\n",
    "    for num in range(max_player):\n",
    "        feature_data1 += filled_df1.loc[num, features].tolist() \n",
    "        feature_data2 += filled_df2.loc[num, features].tolist() \n",
    "        \n",
    "    new_data.loc[i, new_features + result_features] = feature_data1 + feature_data2 + [score_ourside, score_opposite, result]\n",
    "\n",
    "new_data.to_csv('./data/nba_mvp1.csv')\n",
    "\n",
    "print(len(new_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file_path = 'data/nba/basketball_reference_part2_finish/2023_12_27/2023_12_27_OKC_vs_NYK_basic_stats.csv'\n",
    "# file_path = 'data/nba/basketball_reference/2000_01_26/2000_01_26_MIL_vs_SAC_basic_stats.csv'\n",
    "# df_basic = pd.read_csv(file_path).drop_duplicates()\n",
    "# print(df_basic['PTS'].dtype=='object')\n",
    "# # print(df_basic['PTS'].sum())\n",
    "# df_basic = df_basic.drop(df_basic.index[-1])\n",
    "# print(df_basic['PTS'].astype(float).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练胜负模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import zipfile\n",
    "import os \n",
    "import py7zr\n",
    "import multiprocessing \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "# from plot_helper import dependence_plot_v12\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error, classification_report\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "DRtg_bins = 3\n",
    "ORtg_bins = 20\n",
    "DRtg_Fuzzification = True\n",
    "DRtg_Fuzzification = False\n",
    "ORtg_Fuzzification = True\n",
    "# ORtg_Fuzzification = False\n",
    "def data_load(data):\n",
    "    df = data\n",
    "    # print(df.info())\n",
    "    # print(df.columns)\n",
    "    # 将时间转换为浮点数\n",
    "    time_features = []\n",
    "    for i in range(36):\n",
    "        feature = 'MP' + '_' + str(i)\n",
    "        time_features.append(feature)\n",
    "    # print(time_features) \n",
    "    \n",
    "    df = df[~df['BPM_0'].astype(str).str.contains(':')]\n",
    "    # print(df['BPM_0'].astype(str).str.contains(':'))\n",
    "    print('清洗后数据：',len(df))\n",
    "    df[time_features] = df[time_features].fillna('0:0')\n",
    "    df[time_features] = df[time_features].map(lambda x: '0:0' if ':' not in str(x) else x)\n",
    "    df[time_features] =df[time_features].map(lambda x: float(x.replace(':', '.')))\n",
    "    \n",
    "    if  DRtg_Fuzzification:\n",
    "        #  #缩小'ORtg','DRtg'影响\n",
    "        norm_features = []\n",
    "        for i in range(36):\n",
    "            for feat in ['DRtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "        df = df.fillna(0)\n",
    "        # # 转换为数值类型\n",
    "        df[norm_features] = df[norm_features].apply(pd.to_numeric, errors='coerce')\n",
    "        # 将负值变为0\n",
    "        df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n",
    "        # 将 norm_features 值聚合在一起\n",
    "        norm_feature_values = df[norm_features].values.flatten().reshape(-1, 1)    \n",
    "        print(norm_feature_values.shape)\n",
    "        # 将数据划分为 10 个区间进行数值离散化处理\n",
    "        kdb = KBinsDiscretizer(n_bins = DRtg_bins, encode = 'ordinal', strategy='quantile')\n",
    "        kdb.fit(norm_feature_values)\n",
    "        for norm_feature in norm_features:\n",
    "            df[norm_feature] = kdb.transform(df[norm_feature].values.reshape(-1, 1))\n",
    "    if  ORtg_Fuzzification:\n",
    "        norm_features = []\n",
    "        for i in range(36):\n",
    "            for feat in ['ORtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "        # # 转换为数值类型\n",
    "        df[norm_features] = df[norm_features].apply(pd.to_numeric, errors='coerce')\n",
    "        # 将负值变为0\n",
    "        df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n",
    "        # 将 norm_features 值聚合在一起\n",
    "        norm_feature_values = df[norm_features].values.flatten().reshape(-1, 1)    \n",
    "        print(norm_feature_values.shape)\n",
    "        # 将数据划分为 10 个区间进行数值离散化处理\n",
    "        kdb = KBinsDiscretizer(n_bins = ORtg_bins, encode = 'ordinal', strategy='quantile')\n",
    "        kdb.fit(norm_feature_values)\n",
    "        for norm_feature in norm_features:\n",
    "            df[norm_feature] = kdb.transform(df[norm_feature].values.reshape(-1, 1))\n",
    "        # print(df[['DRtg_0','DRtg_35']])\n",
    "        # print(df[['ORtg_0','ORtg_35']])\n",
    "    print(df.info())\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "def data_prerpocess(df, training = True):\n",
    "    \n",
    "    #V_0：全有\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_1去除+-，DRtg\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'BPM']\n",
    "    #V_2去除MP,+-，DRtg\n",
    "    # used_features = ['FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','BPM']\n",
    "    #v_3 +-,'BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','DRtg']\n",
    "    # V_4去除PTS,+-,'ORtg','BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_5 去除+-,'ORtg','DRtg', 'BPM'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_6 去除+-,'ORtg','DRtg', 'BPM','PTS'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_7 缩小+-,'BPM','ORtg','DRtg', \n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_8 去除+-,'BPM','ORtg',通过模糊化降低'DRtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_9 去除+-,'BPM',通过模糊化降低'DRtg','ORtg'影响值\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg','ORtg']\n",
    "    #v_10 去除+-,通过模糊化降低'DRtg','ORtg'影响值\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%','ORtg', 'DRtg', 'BPM']\n",
    "    \n",
    "    total_over_features = []\n",
    "    for i in range(36):\n",
    "        for feature in used_features:\n",
    "            feature = feature + '_' + str(i)\n",
    "            total_over_features.append(feature)\n",
    "    print('选择的特征数量：',len(total_over_features))\n",
    "    X = df[total_over_features].astype(float)\n",
    "    Y = df['result'].astype(int)\n",
    "    print('处理后数据：',len(X))\n",
    "    \n",
    "    if training:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) \n",
    "        return X_train[total_over_features], X_test[total_over_features], Y_train, Y_test\n",
    "    else:\n",
    "        return X[total_over_features], Y, total_over_features\n",
    "\n",
    "def lgb_train(X_train, X_test, y_train, y_test):\n",
    "    norm_features = []\n",
    "    if  DRtg_Fuzzification:\n",
    "        for i in range(36):\n",
    "            for feat in ['DRtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "    if  ORtg_Fuzzification:\n",
    "        for i in range(36):\n",
    "            for feat in ['ORtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "    categorical_feature = norm_features\n",
    "    model = lightgbm.LGBMClassifier(objective='binary', learning_rate=0.1, num_leaves = 30, min_data_in_leaf = 100, feature_fraction = 0.9, max_depth = 8, n_estimators=1500)\n",
    "    callbacks = [log_evaluation(period=1), early_stopping(stopping_rounds=10)]\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='binary_logloss', callbacks = callbacks, categorical_feature = categorical_feature)\n",
    "    joblib.dump(model, 'model/nba_mvp_lgb_v12.pkl')\n",
    "    model = joblib.load('model/nba_mvp_lgb_v12.pkl')\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "\n",
    "\n",
    "    print(np.average(y_pred))\n",
    "    print('The acc of prediction is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "    print('The precision of prediction is {}'.format(precision_score(y_test, y_pred)))\n",
    "    print('The recall of prediction is {}'.format(recall_score(y_test, y_pred)))\n",
    "\n",
    "    X = pd.concat((X_train, X_test))\n",
    "    y = np.append(y_train, y_test)\n",
    "    y_all_pred = model.predict(X, num_iteration=model.best_iteration_)\n",
    "    print('The acc of all prediction is {}'.format(accuracy_score(y, y_all_pred)))\n",
    "    print('The precision of all prediction is {}'.format(precision_score(y, y_all_pred)))\n",
    "    print('The recall of all prediction is {}'.format(recall_score(y, y_all_pred)))\n",
    "\n",
    "def rawdata_load(extract_folder):\n",
    "    file_path_list = []\n",
    "    df_list = []\n",
    "    # 遍历解压后的文件夹，并读取每个 CSV 文件\n",
    "    for root, dirs, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\"basic_stats.csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_path_list.append(file_path)\n",
    "                # print(file_path)\n",
    "                # 使用 pandas 读取 CSV 文件\n",
    "                df_basic = pd.read_csv(file_path).drop_duplicates()\n",
    "                df_advanced = pd.read_csv(file_path.replace('basic_stats.csv','advanced_stats.csv')).drop_duplicates()\n",
    "                if df_basic['PTS'].dtype=='object':\n",
    "                    df_basic = df_basic.drop(df_basic.index[-1])\n",
    "                    df_advanced = df_advanced.drop(df_advanced.index[-1])\n",
    "                # 连接两个文件：63162\n",
    "                df = pd.concat([df_basic, df_advanced], axis=1)\n",
    "                df = df.loc[:,~df.columns.duplicated()]\n",
    "                # 将文件名作为新列添加到 DataFrame\n",
    "                df['file_name'] = file_path.split('/')[-1]\n",
    "                df_list.append(df)\n",
    "    print(len(file_path_list),len(df_list)) \n",
    "    max_player = 18\n",
    "\n",
    "    features = ['starters', 'team', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "        'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "        'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM',\n",
    "        'file_name']\n",
    "    # 连接所有球员数据\n",
    "    new_features = []\n",
    "    for i in range(max_player*2):\n",
    "        for feat in features:\n",
    "            new_feat = feat + '_' + str(i)\n",
    "            new_features.append(new_feat)\n",
    "    # print(len(features), len(new_features))\n",
    "\n",
    "    result_features = ['score_ourside', 'score_opposite', 'result']\n",
    "\n",
    "    new_data = pd.DataFrame(columns = new_features + result_features)\n",
    "    for i in tqdm(range(len(file_path_list))):\n",
    "        # for i in range(10):\n",
    "        path = file_path_list[i]\n",
    "        # print(path.split('/')[-1])\n",
    "        df1 = df_list[i]\n",
    "        filled_df1 = df1.reindex(range(max_player))\n",
    "        #对手球队数据\n",
    "        a = path.split('/')[-1].split('_')[3]\n",
    "        b = path.split('/')[-1].split('_')[5]\n",
    "        oppo_path = path.replace(a+'_vs_'+b, b+'_vs_'+a)\n",
    "        # print(oppo_path,file_path_list.index(oppo_path))\n",
    "        df2 = df_list[file_path_list.index(oppo_path)]\n",
    "        filled_df2 = df2.reindex(range(max_player))\n",
    "        # print(filled_df1[\"PTS\"],filled_df2[\"PTS\"])\n",
    "        score_ourside = filled_df1[\"PTS\"].astype(float).sum()\n",
    "        score_opposite = filled_df2[\"PTS\"].astype(float).sum()\n",
    "        if score_ourside > score_opposite:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "        \n",
    "        feature_data1 = []\n",
    "        feature_data2 = []\n",
    "        for num in range(max_player):\n",
    "            feature_data1 += filled_df1.loc[num, features].tolist() \n",
    "            feature_data2 += filled_df2.loc[num, features].tolist() \n",
    "            \n",
    "        new_data.loc[i, new_features + result_features] = feature_data1 + feature_data2 + [score_ourside, score_opposite, result]\n",
    "    return new_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = './data/nba_mvp.csv'\n",
    "    df1 = pd.read_csv(data_path)\n",
    "    #加上24年数据\n",
    "    folder_path = 'data/basketball_data_info/'\n",
    "    df2 = rawdata_load(folder_path)\n",
    "    df = pd.concat([df1, df2], axis=0)  \n",
    "    print(len(df1),len(df2),len(df))\n",
    "    data = data_load(df)\n",
    "    X_train, X_test, y_train, y_test = data_prerpocess(data)\n",
    "    lgb_train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征shap value值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2397963/967433280.py:357: DtypeWarning: Columns (37,74,111,148,185,222,259,296,333,370,407,444,481,495,496,497,532,533,534,535,570,571,572,573,608,609,610,611,646,647,648,649,684,721,758,795,832,869,906,943,980,1017,1054,1091,1128,1165,1179,1180,1181,1216,1217,1218,1219,1254,1255,1256,1257,1292,1293,1294,1295,1330,1331,1332,1333,1368) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后数据： 63112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2397963/967433280.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[time_features] = df[time_features].fillna('0:0')\n",
      "/tmp/ipykernel_2397963/967433280.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[time_features] = df[time_features].map(lambda x: '0:0' if ':' not in str(x) else x)\n",
      "/tmp/ipykernel_2397963/967433280.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[time_features] =df[time_features].map(lambda x: float(x.replace(':', '.')))\n",
      "/tmp/ipykernel_2397963/967433280.py:50: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2272032, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvp_shap/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2397963/967433280.py:68: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2272032, 1)\n",
      "       DRtg_0  DRtg_35\n",
      "0         3.0      0.0\n",
      "1         4.0      0.0\n",
      "2         1.0      0.0\n",
      "3         3.0      0.0\n",
      "4         4.0      0.0\n",
      "...       ...      ...\n",
      "63157     3.0      0.0\n",
      "63158     3.0      0.0\n",
      "63159     1.0      0.0\n",
      "63160     0.0      0.0\n",
      "63161     1.0      0.0\n",
      "\n",
      "[63112 rows x 2 columns]\n",
      "       +/-_0  +/-_35\n",
      "0        0.0     0.0\n",
      "1        0.0     0.0\n",
      "2        0.0     0.0\n",
      "3        0.0     0.0\n",
      "4        0.0     0.0\n",
      "...      ...     ...\n",
      "63157    0.0     0.0\n",
      "63158    0.0     0.0\n",
      "63159    0.0     0.0\n",
      "63160    0.0     0.0\n",
      "63161    0.0     0.0\n",
      "\n",
      "[63112 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63112 entries, 0 to 63161\n",
      "Columns: 1372 entries, Unnamed: 0 to result\n",
      "dtypes: float64(1238), int64(2), object(132)\n",
      "memory usage: 661.1+ MB\n",
      "None\n",
      "Index(['Unnamed: 0', 'starters_0', 'team_0', 'MP_0', 'FG_0', 'FGA_0', 'FG%_0',\n",
      "       '3P_0', '3PA_0', '3P%_0',\n",
      "       ...\n",
      "       'BLK%_35', 'TOV%_35', 'USG%_35', 'ORtg_35', 'DRtg_35', 'BPM_35',\n",
      "       'file_name_35', 'score_ourside', 'score_opposite', 'result'],\n",
      "      dtype='object', length=1372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvp_shap/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022比赛数据： 2460\n",
      "选择的特征数量： 1260\n",
      "处理后数据： 2460\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvp_shap/lib/python3.9/site-packages/shap/explainers/_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2460, 1260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2460/2460 [00:00<00:00, 5762.90it/s]\n",
      "100%|██████████| 246/246 [01:31<00:00,  2.70it/s]\n",
      "100%|██████████| 246/246 [01:58<00:00,  2.08it/s]\n",
      "100%|██████████| 246/246 [02:07<00:00,  1.93it/s]\n",
      "100%|██████████| 246/246 [02:16<00:00,  1.80it/s]\n",
      "100%|██████████| 246/246 [02:16<00:00,  1.80it/s]\n",
      "100%|██████████| 246/246 [02:17<00:00,  1.79it/s]\n",
      "100%|██████████| 246/246 [02:18<00:00,  1.77it/s]\n",
      "100%|██████████| 246/246 [02:18<00:00,  1.77it/s]\n",
      "100%|██████████| 246/246 [02:19<00:00,  1.76it/s]\n",
      "100%|██████████| 246/246 [02:20<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "#DRtg=5, ORtg不需要，效果可以\n",
    "year = 2022\n",
    "DRtg_bins = 8\n",
    "PM_bins = 3\n",
    "DRtg_Fuzzification = True\n",
    "# DRtg_Fuzzification = False\n",
    "PM_Fuzzification = True\n",
    "# PM_Fuzzification = False\n",
    "\n",
    "def data_load(data):\n",
    "    df = data\n",
    "    # print(df.info())\n",
    "    # print(df.columns)\n",
    "    # 将时间转换为浮点数\n",
    "    time_features = []\n",
    "    for i in range(36):\n",
    "        feature = 'MP' + '_' + str(i)\n",
    "        time_features.append(feature)\n",
    "    # print(time_features) \n",
    "    \n",
    "    df = df[~df['BPM_0'].astype(str).str.contains(':')]\n",
    "    # print(df['BPM_0'].astype(str).str.contains(':'))\n",
    "    print('清洗后数据：',len(df))\n",
    "    df[time_features] = df[time_features].fillna('0:0')\n",
    "    df[time_features] = df[time_features].map(lambda x: '0:0' if ':' not in str(x) else x)\n",
    "    df[time_features] =df[time_features].map(lambda x: float(x.replace(':', '.')))\n",
    "    df = df.fillna(0)\n",
    "    if  DRtg_Fuzzification:\n",
    "        #  #缩小'ORtg','DRtg'影响\n",
    "        norm_features = []\n",
    "        for i in range(36):\n",
    "            for feat in ['DRtg']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "        # df = df.fillna(0)\n",
    "        # # 转换为数值类型\n",
    "        df[norm_features] = df[norm_features].apply(pd.to_numeric, errors='coerce')\n",
    "        # 将负值变为0\n",
    "        df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n",
    "        # 将 norm_features 值聚合在一起\n",
    "        norm_feature_values = df[norm_features].values.flatten().reshape(-1, 1)    \n",
    "        print(norm_feature_values.shape)\n",
    "        # 将数据划分为 10 个区间进行数值离散化处理\n",
    "        kdb = KBinsDiscretizer(n_bins = DRtg_bins, encode = 'ordinal', strategy='quantile')\n",
    "        kdb.fit(norm_feature_values)\n",
    "        for norm_feature in norm_features:\n",
    "            df[norm_feature] = kdb.transform(df[norm_feature].values.reshape(-1, 1))\n",
    "    if  PM_Fuzzification:\n",
    "        norm_features = []\n",
    "        for i in range(36):\n",
    "            for feat in ['+/-']:\n",
    "                feature = feat + '_' + str(i)\n",
    "                norm_features.append(feature)\n",
    "        # # 转换为数值类型\n",
    "        df[norm_features] = df[norm_features].apply(pd.to_numeric, errors='coerce')\n",
    "        # 将负值变为0\n",
    "        df[norm_features] = df[norm_features].applymap(lambda x: 0 if x < 0 else x)\n",
    "        # 将 norm_features 值聚合在一起\n",
    "        norm_feature_values = df[norm_features].values.flatten().reshape(-1, 1)    \n",
    "        print(norm_feature_values.shape)\n",
    "        # 将数据划分为 10 个区间进行数值离散化处理\n",
    "        kdb = KBinsDiscretizer(n_bins = PM_bins, encode = 'ordinal', strategy='quantile')\n",
    "        kdb.fit(norm_feature_values)\n",
    "        for norm_feature in norm_features:\n",
    "            df[norm_feature] = kdb.transform(df[norm_feature].values.reshape(-1, 1))\n",
    "    print(df[['DRtg_0','DRtg_35']])\n",
    "    print(df[['+/-_0','+/-_35']])\n",
    "    print(df.info())\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "def data_prerpocess(df, training = True):\n",
    "    \n",
    "    #V_0：全有\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_1去除+-，DRtg\n",
    "    used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'BPM']\n",
    "    #V_2去除MP,+-，DRtg\n",
    "    # used_features = ['FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','BPM']\n",
    "    #v_3 +-,'BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','DRtg']\n",
    "    # V_4去除PTS,+-,'ORtg','BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_5 去除+-,'ORtg','DRtg', 'BPM'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_6 去除+-,'ORtg','DRtg', 'BPM','PTS'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_7 缩小+-,'BPM','ORtg','DRtg', \n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_8 去除+-,'BPM','ORtg',通过模糊化降低'DRtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_9 去除+-,'BPM',通过模糊化降低'DRtg','ORtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg','ORtg']\n",
    "    #v_10 去除+-,通过模糊化降低'DRtg','ORtg'影响值\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%','ORtg', 'DRtg', 'BPM']\n",
    "    #v_11 模糊化+-,'DRtg'\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    total_over_features = []\n",
    "    for i in range(36):\n",
    "        for feature in used_features:\n",
    "            feature = feature + '_' + str(i)\n",
    "            total_over_features.append(feature)\n",
    "    \n",
    "    print('选择的特征数量：',len(total_over_features))\n",
    "    X = df[total_over_features].astype(float)\n",
    "    Y = df['result'].astype(int)\n",
    "    X_list.append(X)\n",
    "    Y_list.append(Y)\n",
    "    # print('处理后数据：',len(X))\n",
    " \n",
    "    # #打乱同一个队球员顺序添加训练数据进去\n",
    "    # random.seed(0)\n",
    "    # for sample in range(15):\n",
    "    #     total_over_features = []\n",
    "    #     for i in random.sample(range(18), 18):  # 打乱顺序\n",
    "    #         for feature in used_features:\n",
    "    #             new_feature = feature + '_' + str(i)\n",
    "    #             total_over_features.append(new_feature)\n",
    "                \n",
    "    #     for i in random.sample(range(18,36), 18):  # 打乱顺序\n",
    "    #         for feature in used_features:\n",
    "    #             new_feature = feature + '_' + str(i)\n",
    "    #             total_over_features.append(new_feature)\n",
    "                \n",
    "    #     print('特征顺序：',total_over_features)\n",
    "    #     X1 = df[total_over_features].astype(float)\n",
    "    #     Y1 = df['result'].astype(int)\n",
    "    #     X_list.append(X1)\n",
    "    #     Y_list.append(Y1)\n",
    "        \n",
    "    X = pd.concat(X_list, ignore_index=True)\n",
    "    Y = pd.concat(Y_list, ignore_index=True)    \n",
    "    print('处理后数据：',len(X))\n",
    "    \n",
    "    if training:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) \n",
    "        return X_train[total_over_features], X_test[total_over_features], Y_train, Y_test\n",
    "    else:\n",
    "        return X[total_over_features], Y, total_over_features\n",
    "\n",
    "def compute_shap_values(shap_values_path):\n",
    "    shap_values = np.load(shap_values_path)\n",
    "    feat_num = len(shap_values[0]) // 36 #每个球员有33个特征\n",
    "    mvp_values = []\n",
    "    for i in tqdm(range(len(X))):\n",
    "        shap_value = shap_values[i]\n",
    "        mvp_value = 0\n",
    "        for j in range(len(shap_value)):\n",
    "            mvp_value += shap_value[j]\n",
    "            if j % feat_num == feat_num - 1:\n",
    "                mvp_values.append(mvp_value)\n",
    "                mvp_value = 0\n",
    "    abs_array = np.abs(np.array(mvp_values).reshape(-1,36))\n",
    "    sum_first_three = np.sum(abs_array[:, :18], axis=1, keepdims=True)\n",
    "    x = 0.5\n",
    "    normalized_first_three = (abs_array[:, :18] / sum_first_three) * x\n",
    "\n",
    "    # 后三列数值除以后三列的和并乘以系数（1-x）\n",
    "    sum_last_three = np.sum(abs_array[:, 18:], axis=1, keepdims=True)\n",
    "    normalized_last_three = (abs_array[:, 18:] / sum_last_three) * (1 - x)\n",
    "\n",
    "    # 合并处理后的数组\n",
    "    result_array = np.hstack((normalized_first_three, normalized_last_three))\n",
    "    return result_array, np.array(mvp_values).reshape(-1,36), shap_values\n",
    "\n",
    "def explain(X, model_path, shap_file_name):\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    print(explainer.expected_value)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    \n",
    "    np.save(shap_file_name, shap_values)\n",
    "    shap_values = np.load(shap_file_name)\n",
    "    print(shap_values.shape)\n",
    "# 队内排名\n",
    "def rank(values, order = 1):\n",
    "    if order == 1:\n",
    "        sorted_indices = np.argsort(values)[::-1]\n",
    "    else:\n",
    "        sorted_indices = np.argsort(values)\n",
    "    ranks = np.empty_like(sorted_indices)\n",
    "    ranks[sorted_indices] = np.arange(1, len(sorted_indices) + 1)\n",
    "\n",
    "    return ranks, sorted_indices[0]\n",
    "\n",
    "def process_data(args):\n",
    "    df, feat_values, shap_values, id = args\n",
    "    #V_0：全有\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_1去除+-，DRtg\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'BPM']\n",
    "    #V_2去除MP,+-，DRtg\n",
    "    # used_features = ['FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg','BPM']\n",
    "    #V_3 +-,'ORtg','BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    # V_4去除PTS,+-,'ORtg','BPM'\n",
    "    # used_features = ['MP','FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_5 去除+-,'ORtg','DRtg', 'BPM'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_6 去除+-,'ORtg','DRtg', 'BPM','PTS'\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #      'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%']\n",
    "    #v_7 缩小+-,'BPM','ORtg','DRtg', \n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    #v_8 去除+-,'BPM','ORtg',通过模糊化降低'DRtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg']\n",
    "    #v_9 去除+-,'BPM',通过模糊化降低'DRtg','ORtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'DRtg','ORtg']\n",
    "    #v_10 去除+-,通过模糊化降低'DRtg','ORtg'影响值\n",
    "    # used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "    #    'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "    #     'PTS', 'TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "    #    'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%','ORtg', 'DRtg', 'BPM']\n",
    "    #v_12 模糊化+-,'DRtg'\n",
    "    used_features = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT',\n",
    "       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
    "        'PTS', '+/-','TS%', 'eFG%', '3PAr', 'FTr', 'ORB%', 'DRB%',\n",
    "       'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "    \n",
    "    feature_value = []\n",
    "    id_feature = 'starters'\n",
    "    id_features = [id_feature + '_' + str(i) for i in range(36)]\n",
    "    \n",
    "\n",
    "    score_features = ['score_ourside', 'score_opposite']\n",
    "    total_over_features = []\n",
    "    feat_num = len(used_features)\n",
    "    \n",
    "    for feature in used_features:\n",
    "        feature_value.append(feature)\n",
    "        feature_value.append(feature + '_shap_value')\n",
    "    value_name = [feature_value[i] for i in range(1, len(feature_value), 2)]\n",
    "\n",
    "    for i in range(36):\n",
    "        for feature in used_features:\n",
    "            feature = feature + '_' + str(i)\n",
    "            total_over_features.append(feature)\n",
    "  \n",
    "    #ds, role_id\n",
    "    new_data = pd.DataFrame(columns = ['file_name', 'result', 'starters', 'home_team', 'team_score'] + feature_value + [ 'shap_score', 'shap_order_team'])\n",
    "    for i in tqdm(range(len(df))):\n",
    "        result = df.loc[i, 'result']\n",
    "        match_id = df.loc[i, 'file_name_0']\n",
    "      \n",
    "        self_shap_values = shap_values[i][:18]\n",
    "        opp_shap_values = shap_values[i][18:]\n",
    "        \n",
    "        for j in range(18):\n",
    "            #ds, role_id, feat_shap_value\n",
    "            new_data.loc[36 * i + j, ['file_name', 'result', 'starters', 'team_score'] + used_features] = df.loc[i, ['file_name_0', 'result',  id_features[j], score_features[0]] + total_over_features[j * feat_num : (j + 1) * feat_num]].tolist()\n",
    "            new_data.loc[36 * i + j, 'home_team'] = '1'\n",
    "            new_data.loc[36 * i + j, value_name] = feat_values[i][j * feat_num : (j + 1) * feat_num] #n * feat_num * 6\n",
    "            \n",
    "        rows = [36 * i + n for n in range(18)]\n",
    "        shap_ranks, shap_index1 = rank(self_shap_values)\n",
    "        new_data.loc[rows, 'shap_order_team'] = shap_ranks\n",
    "        \n",
    "        \n",
    "        for j in range(18, 36):\n",
    "            new_data.loc[36 * i + j, ['file_name',  'starters', 'team_score'] + used_features] = df.loc[i, ['file_name_18', id_features[j], score_features[1]] + total_over_features[j * feat_num : (j + 1) * feat_num]].tolist()\n",
    "            new_data.loc[36 * i + j, 'home_team'] = '0'\n",
    "            new_data.loc[36 * i + j, 'result'] = 1 - df.loc[i, 'result']\n",
    "            new_data.loc[36 * i + j, value_name] = feat_values[i][j * feat_num : (j + 1) * feat_num]\n",
    "           \n",
    "        rows = [36 * i + n for n in range(18,36)]\n",
    "        shap_ranks, shap_index2 = rank(opp_shap_values, -1)\n",
    "        new_data.loc[rows, 'shap_order_team'] = shap_ranks\n",
    "\n",
    "    new_data['shap_score'] = shap_values.reshape(-1)\n",
    "    return new_data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    data_path = './data/nba_mvp.csv'\n",
    "    model_path = 'model/nba_mvp_lgb_v12.pkl'\n",
    "    season = 'Regular_season/'\n",
    "    # season = 'Finals/'\n",
    "    df = pd.read_csv(data_path)\n",
    "    data = data_load(df)\n",
    "    \n",
    "    if year == 2024:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2024_04_10|2024_04_11|2024_04_12|2024_04_13|2024_04_14'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2023:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2022:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2022_04_10'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]    \n",
    "    if year == 2021:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_12|{}_01|{}_02|{}_03|{}_04|{}_05_0|2021_05_10|2021_05_11|2021_05_12|2021_05_13|2021_05_14|2021_05_15|2021_05_16'\n",
    "                        .format(year-1,year,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2020:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04|{}_07|{}_08_0|2020_08_10|2020_08_11|2020_08_12|2020_08_13|2020_08_14'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2019:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2019_04_10'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2018:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2018_04_10|2018_04_11'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2017:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2017_04_10|2017_04_11|2017_04_12'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2016:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2016_04_10|2016_04_11|2016_04_12|2016_04_13'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2015:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2015_04_10|2015_04_11|2015_04_12|2015_04_13|2015_04_14|2015_04_15'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2014:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2014_04_10|2014_04_11|2014_04_12|2014_04_13|2014_04_14|2014_04_15|2014_04_16'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2013:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2013_04_10|2013_04_11|2013_04_12|2013_04_13|2013_04_14|2013_04_15|2013_04_16|2013_04_17'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2012:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|{}_04_1|2012_04_20|2012_04_21|2012_04_22|2012_04_23|2012_04_24|2012_04_25|2012_04_26'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2011:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2011_04_10|2011_04_11|2011_04_12|2011_04_13'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2010:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2010_04_10|2010_04_11|2010_04_12|2010_04_13|2010_04_14'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2009:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2009_04_10|2009_04_11|2009_04_12|2009_04_13|2009_04_14|2009_04_15'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2008:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2008_04_10|2008_04_11|2008_04_12|2008_04_13|2008_04_14|2008_04_15|2008_04_16'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2007:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|2007_04_10|2007_04_11|2007_04_12|2007_04_13|2007_04_14|2007_04_15|2007_04_16|2007_04_17|2007_04_18'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year), regex=True)).any(axis=1)]\n",
    "    if year == 2006:\n",
    "        data = data[data.apply(lambda x: x.astype(str).str.contains('{}_10|{}_11|{}_12|{}_01|{}_02|{}_03|{}_04_0|{}_04_1'\n",
    "                        .format(year-1,year-1,year-1,year,year,year,year,year), regex=True)).any(axis=1)]\n",
    "        \n",
    " \n",
    "    print('{}比赛数据：'.format(year),len(data))\n",
    "    X, Y, total_over_features = data_prerpocess(data, training= False) \n",
    "    shap_file_name = './shap/'+season+'{}_nba_mvp_shap_values_v12.npy'.format(year)\n",
    "    # if not os.path.exists(shap_file_name):\n",
    "    explain(X, model_path, shap_file_name)\n",
    "    money_values, shap_values, feat_values = compute_shap_values(shap_file_name)\n",
    "\n",
    "    # 分割数据\n",
    "    num_cores = 10\n",
    "    chunk_size = len(X) // num_cores\n",
    "    chunks = [(data[i:i + chunk_size].reset_index(drop = True), feat_values[i : i + chunk_size], shap_values[i : i + chunk_size], i // chunk_size * 100 // num_cores) for i in range(0, len(data), chunk_size)]\n",
    "    # 使用线程池处理数据\n",
    "\n",
    "    # 创建进程池，并利用多个核心进行并行处理\n",
    "    with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "        results = pool.map(process_data, chunks)\n",
    "\n",
    "    # 合并处理结果\n",
    "    data_list = [results[i] for i in range(num_cores)]\n",
    "    \n",
    "    processed_data = pd.concat(data_list).reset_index(drop = True)\n",
    "    processed_data.to_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征分析图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制summary分析图并保存\n",
    "def summary_bar_plot(shap_values, max_display=20, show=False, save_path=None):\n",
    "    plt.style.use('ggplot')\n",
    "    shap.plots.bar(shap_values, max_display=max_display, show=show)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.encode(encoding='utf-8'), dpi=200, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "def summary_dot_plot(shap_values, max_display=20, show=False, save_path=None):\n",
    "    plt.style.use('ggplot')\n",
    "    shap.plots.beeswarm(shap_values, max_display=max_display, show=show)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.encode(encoding='utf-8'), dpi=200, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "# 绘制summary分析图并保存\n",
    "save_path = 'shap/'+season+'{}_nba_mvp_summary_dot_v12.png'.format(year)\n",
    "summary_dot_plot(shap.Explanation(\n",
    "values=feat_values, \n",
    "# base_values = 0,\n",
    "data=np.array(X),\n",
    "feature_names=total_over_features), max_display=50, show=False, save_path=save_path)\n",
    "\n",
    "save_path = 'shap/'+season+'{}_nba_mvp_summary_bar_v12.png'.format(year)\n",
    "summary_bar_plot(shap.Explanation(\n",
    "values=feat_values, \n",
    "# base_values = np.array(0),\n",
    "data=np.array(X),\n",
    "feature_names=total_over_features), max_display=50, show=False, save_path=save_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mvp评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "# MRR (Mean Reciprocal Rank)\n",
    "def mean_reciprocal_rank(ranking):\n",
    "    for i, rank in enumerate(ranking):\n",
    "        if rank == 1:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "season = 'Regular_season/'\n",
    "year = 2024\n",
    "#2024\n",
    "if year == 2024:\n",
    "    mvp_voting = ['Nikola Jokić', 'Shai Gilgeous-Alexander', 'Luka Dončić','Giannis Antetokounmpo',\n",
    "                  'Jalen Brunson','Jayson Tatum', 'Anthony Edwards', 'Domantas Sabonis', \n",
    "                  'Kevin Durant']\n",
    "#2023\n",
    "if year == 2023:\n",
    "    mvp_voting = ['Joel Embiid', 'Nikola Jokić', 'Giannis Antetokounmpo', 'Jayson Tatum', \n",
    "                  'Shai Gilgeous-Alexander', 'Donovan Mitchell', 'Domantas Sabonis', 'Luka Dončić'\n",
    "                  , 'Stephen Curry', 'Jimmy Butler', \"De'Aaron Fox\", 'Jalen Brunson', 'Ja Morant']\n",
    "#2022\n",
    "if year == 2022:\n",
    "    mvp_voting = ['Nikola Jokić', 'Joel Embiid', 'Giannis Antetokounmpo', 'Devin Booker',\n",
    "                  'Luka Dončić', 'Jayson Tatum', 'Ja Morant', 'Stephen Curry', 'Chris Paul',\n",
    "                  'DeMar DeRozan', 'Kevin Durant', 'LeBron James']\n",
    "#2021\n",
    "if year == 2021:\n",
    "    mvp_voting = ['Nikola Jokić', 'Joel Embiid', 'Stephen Curry', 'Giannis Antetokounmpo',\n",
    "                  'Chris Paul', 'Luka Dončić', 'Damian Lillard', 'Julius Randle', 'Derrick Rose',\n",
    "                  'Rudy Gobert', 'Russell Westbrook', 'Ben Simmons', 'James Harden','LeBron James', 'Kawhi Leonard']\n",
    "#2020\n",
    "if year == 2020:\n",
    "    mvp_voting = ['Giannis Antetokounmpo', 'LeBron James', 'James Harden', 'Luka Dončić',\n",
    "    'Kawhi Leonard', 'Anthony Davis', 'Chris Paul','Damian Lillard','Nikola Jokić',\n",
    "    'Pascal Siakam','Jimmy Butler','Jayson Tatum']\n",
    "#2019\n",
    "if year == 2019:\n",
    "    mvp_voting = ['Giannis Antetokounmpo', 'James Harden', 'Paul George', 'Nikola Jokić',\n",
    "    'Stephen Curry','Damian Lillard','Joel Embiid','Kevin Durant','Kawhi Leonard','Russell Westbrook',\n",
    "    'Rudy Gobert','LeBron James']\n",
    "#2018\n",
    "if year == 2018:\n",
    "    mvp_voting = ['James Harden', 'LeBron James', 'Anthony Davis', 'Damian Lillard',              \n",
    "    'Russell Westbrook','Giannis Antetokounmpo','Kevin Durant','DeMar DeRozan','LaMarcus Aldridge',\n",
    "    'Jimmy Butler','Stephen Curry','Joel Embiid','Victor Oladipo']\n",
    "#2017\n",
    "if year == 2017:\n",
    "    mvp_voting = ['Russell Westbrook', 'James Harden', 'Kawhi Leonard', 'LeBron James',\n",
    "    'Isaiah Thomas','Stephen Curry','Giannis Antetokounmpo','John Wall','Anthony Davis',\n",
    "    'Kevin Durant','DeMar DeRozan']\n",
    "#2016\n",
    "if year == 2016:\n",
    "    mvp_voting = ['Stephen Curry', 'Kawhi Leonard', 'LeBron James', 'Russell Westbrook',\n",
    "    'Kevin Durant','Chris Paul','Draymond Green','Damian Lillard','James Harden',\n",
    "    'Kyle Lowry']\n",
    "#2020\n",
    "if year == 2015:\n",
    "    mvp_voting = ['Stephen Curry', 'James Harden', 'LeBron James', 'Russell Westbrook',\n",
    "    'Anthony Davis','Chris Paul','LaMarcus Aldridge','Marc Gasol','Blake Griffin',\n",
    "    'Tim Duncan','Kawhi Leonard','Klay Thompson']\n",
    "#2020\n",
    "if year == 2014:\n",
    "    mvp_voting = ['Kevin Durant', 'LeBron James', 'Blake Griffin', 'Joakim Noah',\n",
    "    'James Harden','Stephen Curry','Chris Paul','Al Jefferson','Paul George','LaMarcus Aldridge',\n",
    "    'Kevin Love','Tim Duncan','Tony Parker','Dirk Nowitzki','Carmelo Anthony',\n",
    "    'Goran Dragić','Mike Conley']\n",
    "#2020\n",
    "if year == 2013:\n",
    "    mvp_voting = ['LeBron James', 'Kevin Durant', 'Carmelo Anthony', 'Chris Paul',\n",
    "    'Kobe Bryant','Tony Parker','Tim Duncan','James Harden','Russell Westbrook','Dwyane Wade',\n",
    "    'Stephen Curry','Kevin Garnett','Marc Gasol','Ty Lawson','David Lee','Joakim Noah']\n",
    "#2020\n",
    "if year == 2012:\n",
    "    mvp_voting = ['LeBron James', 'Kevin Durant', 'Chris Paul', 'Kobe Bryant',\n",
    "    'Tony Parker','Kevin Love','Dwight Howard','Rajon Rondo','Steve Nash','Dwyane Wade',\n",
    "    'Derrick Rose','Dirk Nowitzki','Russell Westbrook','Tim Duncan','Joe Johnson']  \n",
    "#2020\n",
    "if year == 2011:\n",
    "    mvp_voting = ['Derrick Rose', 'Dwight Howard', 'LeBron James', 'Kobe Bryant',\n",
    "    'Kevin Durant','Dirk Nowitzki','Dwyane Wade','Manu Ginóbili',\"Amar'e Stoudemire\",\n",
    "    'Blake Griffin','Rajon Rondo','Tony Parker','Chris Paul']\n",
    "#2020\n",
    "if year == 2010:\n",
    "    mvp_voting = ['LeBron James', 'Kevin Durant', 'Kobe Bryant', 'Dwight Howard','Dwyane Wade',\n",
    "    'Carmelo Anthony','Dirk Nowitzki','Steve Nash','Deron Williams',\"Amar'e Stoudemire\",\n",
    "    'Manu Ginóbili','Chauncey Billups','Chris Bosh','Stephen Jackson','Joe Johnson']\n",
    "#2020\n",
    "if year == 2009:\n",
    "    mvp_voting = ['LeBron James', 'Kobe Bryant', 'Dwyane Wade', 'Dwight Howard', 'Chris Paul',               \n",
    "    'Chauncey Billups','Paul Pierce','Tony Parker','Brandon Roy','Dirk Nowitzki',\n",
    "    'Tim Duncan','Yao Ming']\n",
    "#2020\n",
    "if year == 2008:\n",
    "    mvp_voting = ['Kobe Bryant', 'Chris Paul', 'Kevin Garnett', 'LeBron James','Dwight Howard',\n",
    "    \"Amar'e Stoudemire\", 'Tim Duncan','Tracy McGrady','Steve Nash','Manu Ginóbili','Dirk Nowitzki',\n",
    "    'Deron Williams','Carmelo Anthony','Carlos Boozer','Antawn Jamison','Paul Pierce','Rasheed Wallace']\n",
    "#2020\n",
    "if year == 2007:\n",
    "    mvp_voting = ['Dirk Nowitzki', 'Steve Nash', 'Kobe Bryant', 'Tim Duncan','LeBron James',\n",
    "    'Tracy McGrady','Chris Bosh','Gilbert Arenas','Carlos Boozer','Kevin Garnet','Chauncey Billups',\n",
    "    \"Shaquille O'Neal\",'Dwyane Wade',\"Amar'e Stoudemire\",'Carmelo Anthony','Baron Davis','Tony Parker']\n",
    "#2020\n",
    "if year == 2006:\n",
    "    mvp_voting = ['Steve Nash', 'LeBron James', 'Dirk Nowitzki', 'Kobe Bryant', 'Chauncey Billups',\n",
    "    'Dwyane Wade','Elton Brand','Tim Duncan','Tony Parker','Allen Iverson','Shawn Marion']\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上场人次： 52788\n",
      "180 starters\n",
      "Victor Wembanyama        2.210526\n",
      "Nikola Jokić             2.715596\n",
      "Daniel Gafford           2.933333\n",
      "Rudy Gobert              3.245283\n",
      "Nick Richards            3.357143\n",
      "Nic Claxton              3.518519\n",
      "Giannis Antetokounmpo    3.888889\n",
      "Anthony Davis            3.913043\n",
      "Bam Adebayo              4.825000\n",
      "Zion Williamson          4.857143\n",
      "Name: shap_order_team, dtype: float64\n",
      "rank_mean_err: 26.22222222222222\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1.0, 15.0, 18.0, 6.0, 67.0, 61.0, 29.0, 17.0, 58.0]\n",
      "Spearman相关系数: 0.5833333333333334\n",
      "Precision or recall: 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "#球员在所有胜场的shapley值排名平均\n",
    "topk = 3\n",
    "topk = len(mvp_voting)\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "data_value = pd.read_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "data_value= data_value[data_value['MP'] != 0.0]\n",
    "# print(data_value['MP'])\n",
    "print('上场人次：',len(data_value))\n",
    "# print('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "#最低场次限制 \n",
    "# {24:130, 23:110, 22:109, 21:87, 20:113, 19:109, 18:100, 17:122\n",
    "# 16:130, 15:127, 14:123, 13:130, 12:77, 11:130, 10:130, 09:130\n",
    "# 08:130, 07:79, 06:130}\n",
    "result_df = \\\n",
    "data_value[data_value['starters'].\n",
    "           map(data_value['starters'].value_counts()) > 130]\n",
    "#胜场\n",
    "result_df_win = result_df[result_df['result'] == 1]\n",
    "mvp_by_shap = result_df_win.groupby('starters')['shap_order_team'].mean().sort_values()\n",
    "print(len(mvp_by_shap),mvp_by_shap[:10])\n",
    "# mvp = mvp_by_shap[mvp_by_shap == mvp_by_shap.min()]\n",
    "# print('mvp:',mvp) # Nikola Jokić 尼古拉·约基奇:1 ,Joel Embiid 乔尔·恩比德:7\n",
    "mvp_rank = mvp_by_shap.rank().sort_values()\n",
    "# print('Devin Booker:', mvp_rank['Devin Booker'] ,mvp_by_shap['Devin Booker'])\n",
    "\n",
    "rank_mean_err = 0\n",
    "predicted_labels = []\n",
    "for i in range(topk):\n",
    "    predicted_labels.append(mvp_rank[mvp_voting[i]]-1)\n",
    "    rank_mean_err += abs(mvp_rank[mvp_voting[i]]-i-1)\n",
    "print('rank_mean_err:', rank_mean_err/topk)\n",
    "actual_labels = list(range(1, 1+topk))\n",
    "print(actual_labels)\n",
    "print(predicted_labels)\n",
    "# 计算Spearman相关系数\n",
    "correlation, p_value = spearmanr(actual_labels, predicted_labels)\n",
    "print(\"Spearman相关系数:\", correlation)\n",
    "# print(\"p值:\", p_value)\n",
    "#前13名计算召回率=精度：\n",
    "count = sum(1 for i in predicted_labels if i < topk)\n",
    "print(\"Precision or recall:\", count/topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 starters\n",
      "Nikola Jokić               4.859873\n",
      "Victor Wembanyama          5.239437\n",
      "Rudy Gobert                5.730263\n",
      "Giannis Antetokounmpo      6.369863\n",
      "Jusuf Nurkić               6.506579\n",
      "Shai Gilgeous-Alexander    6.933333\n",
      "Anthony Davis              6.948052\n",
      "Chet Holmgren              7.042683\n",
      "Andre Drummond             7.050633\n",
      "Daniel Gafford             7.054054\n",
      "Name: shap_order_team, dtype: float64\n",
      "rank_mean_err: 23.88888888888889\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0.0, 5.0, 14.0, 3.0, 80.0, 39.0, 31.0, 17.0, 62.0]\n",
      "Spearman相关系数: 0.7166666666666667\n",
      "Precision or recall: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#球员在所有场的shapley值排名平均\n",
    "# data_value = pd.read_csv('./data/{}_nba_mvp_values_v12.csv'.format(year))\n",
    "# result_df = data_value[data_value['starters'].map(data_value['starters'].value_counts()) > 130]\n",
    "\n",
    "mvp_by_shap = result_df.groupby('starters')['shap_order_team'].mean().sort_values()\n",
    "print(len(mvp_by_shap),mvp_by_shap[:10])\n",
    "# mvp = mvp_by_shap[mvp_by_shap == mvp_by_shap.min()]\n",
    "# print('mvp:',mvp) # Nikola Jokić 尼古拉·约基奇:1 ,Joel Embiid 乔尔·恩比德:5\n",
    "\n",
    "mvp_rank = mvp_by_shap.rank().sort_values()\n",
    "rank_mean_err = 0\n",
    "predicted_labels = []\n",
    "for i in range(topk):\n",
    "    predicted_labels.append(mvp_rank[mvp_voting[i]]-1)\n",
    "    rank_mean_err += abs(mvp_rank[mvp_voting[i]]-i-1)\n",
    "print('rank_mean_err:', rank_mean_err/topk)\n",
    "actual_labels = list(range(1, 1+topk))\n",
    "print(actual_labels)\n",
    "print(predicted_labels)\n",
    "# 计算Spearman相关系数\n",
    "correlation, p_value = spearmanr(actual_labels, predicted_labels)\n",
    "print(\"Spearman相关系数:\", correlation)\n",
    "# print(\"p值:\", p_value)\n",
    "#前13名计算召回率=精度：\n",
    "count = sum(1 for i in predicted_labels if i < topk)\n",
    "print(\"Precision or recall:\", count/topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starters\n",
      "Nikola Jokić               1.311513\n",
      "Victor Wembanyama          0.977095\n",
      "Rudy Gobert                0.962410\n",
      "Shai Gilgeous-Alexander    0.831670\n",
      "Anthony Davis              0.604502\n",
      "Giannis Antetokounmpo      0.511388\n",
      "Chet Holmgren              0.481327\n",
      "Bam Adebayo                0.404070\n",
      "Jusuf Nurkić               0.388700\n",
      "Isaiah Hartenstein         0.386903\n",
      "Name: shap_score, dtype: float64\n",
      "rank_mean_err: 20.666666666666668\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0.0, 3.0, 12.0, 5.0, 73.0, 23.0, 20.0, 22.0, 64.0]\n",
      "Spearman相关系数: 0.7666666666666667\n",
      "Precision or recall: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#球员在所有场的shapley值平均\n",
    "# data_value = pd.read_csv('./data/{}_nba_mvp_values_v12.csv'.format(year))\n",
    "# result_df = data_value[data_value['starters'].map(data_value['starters'].value_counts()) > 130]\n",
    "result_df1 = result_df[result_df['home_team'] == 1]\n",
    "result_df2 = result_df[result_df['home_team'] == 0]\n",
    "\n",
    "# print(result_df['starters'].value_counts())\n",
    "mvp_by_shap1 = result_df1.groupby('starters')['shap_score'].mean()\n",
    "mvp_by_shap2 = result_df2.groupby('starters')['shap_score'].mean()\n",
    "# print(len(mvp_by_shap1),mvp_by_shap1[:10])\n",
    "# print(len(mvp_by_shap2),mvp_by_shap2[:10])\n",
    "mvp_by_shap = (mvp_by_shap1 - mvp_by_shap2).sort_values(ascending=False)\n",
    "print(mvp_by_shap[:10])\n",
    "\n",
    "mvp_rank = mvp_by_shap.rank(ascending=False)\n",
    "# print('Devin Booker:', 'rank:', mvp_rank['Devin Booker'] , 'mvp Shapley:', mvp_by_shap['Devin Booker'])\n",
    "\n",
    "rank_mean_err = 0\n",
    "predicted_labels = []\n",
    "for i in range(topk):\n",
    "    predicted_labels.append(mvp_rank[mvp_voting[i]]-1)\n",
    "    rank_mean_err += abs(mvp_rank[mvp_voting[i]]-i-1)\n",
    "print('rank_mean_err:', rank_mean_err/topk)\n",
    "actual_labels = list(range(1, 1+topk))\n",
    "print(actual_labels)\n",
    "print(predicted_labels)\n",
    "# 计算Spearman相关系数\n",
    "correlation, p_value = spearmanr(actual_labels, predicted_labels)\n",
    "print(\"Spearman相关系数:\", correlation)\n",
    "# print(\"p值:\", p_value)\n",
    "#前13名计算召回率=精度：\n",
    "count = sum(1 for i in predicted_labels if i < topk)\n",
    "print(\"Precision or recall:\", count/topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starters\n",
      "Nikola Jokić               1.635002\n",
      "Victor Wembanyama          1.571314\n",
      "Rudy Gobert                1.461918\n",
      "Anthony Davis              1.329768\n",
      "Shai Gilgeous-Alexander    1.316598\n",
      "Bam Adebayo                1.154977\n",
      "Giannis Antetokounmpo      1.044392\n",
      "Daniel Gafford             1.029346\n",
      "Chet Holmgren              0.990396\n",
      "Isaiah Hartenstein         0.866604\n",
      "Name: shap_score, dtype: float64\n",
      "rank_mean_err: 16.555555555555557\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0.0, 4.0, 12.0, 6.0, 56.0, 29.0, 16.0, 14.0, 48.0]\n",
      "Spearman相关系数: 0.75\n",
      "Precision or recall: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2412350/1423034049.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
      "/tmp/ipykernel_2412350/1423034049.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
      "/tmp/ipykernel_2412350/1423034049.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
      "/tmp/ipykernel_2412350/1423034049.py:9: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n",
      "/tmp/ipykernel_2412350/1423034049.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n",
      "/tmp/ipykernel_2412350/1423034049.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n"
     ]
    }
   ],
   "source": [
    "#输球负贡献置零\n",
    "# result_df = pd.read_csv('./data/'+season+'{}_nba_mvp_values_v12.csv'.format(year))\n",
    "result_df1 = result_df[result_df['home_team'] == 1]\n",
    "result_df2 = result_df[result_df['home_team'] == 0]\n",
    "#负场\n",
    "result_df_loss1 = result_df1[result_df1['result'] == 0]\n",
    "result_df_loss1['shap_score'][result_df_loss1['shap_score'] < 0] = 0\n",
    "result_df_loss2 = result_df2[result_df2['result'] == 0]\n",
    "result_df_loss2['shap_score'][result_df_loss2['shap_score'] > 0] = 0\n",
    "result_df1 = pd.concat([result_df_loss1, result_df1[result_df1['result'] == 1]])\n",
    "result_df2 = pd.concat([result_df_loss2, result_df2[result_df2['result'] == 1]])\n",
    "# print(result_df['starters'].value_counts())\n",
    "mvp_by_shap1 = result_df1.groupby('starters')['shap_score'].mean()\n",
    "mvp_by_shap2 = result_df2.groupby('starters')['shap_score'].mean()\n",
    "\n",
    "mvp_by_shap = (mvp_by_shap1 - mvp_by_shap2).sort_values(ascending=False)\n",
    "print(mvp_by_shap[:10])\n",
    "\n",
    "mvp_rank = mvp_by_shap.rank(ascending=False)\n",
    "# print('Devin Booker:', 'rank:', mvp_rank['Devin Booker'] , 'mvp Shapley:', mvp_by_shap['Devin Booker'])\n",
    "\n",
    "rank_mean_err = 0\n",
    "predicted_labels = []\n",
    "for i in range(topk):\n",
    "    predicted_labels.append(mvp_rank[mvp_voting[i]]-1)\n",
    "    rank_mean_err += abs(mvp_rank[mvp_voting[i]]-i-1)\n",
    "print('rank_mean_err:', rank_mean_err/topk)\n",
    "actual_labels = list(range(1, 1+topk))\n",
    "print(actual_labels)\n",
    "print(predicted_labels)\n",
    "# 计算Spearman相关系数\n",
    "correlation, p_value = spearmanr(actual_labels, predicted_labels)\n",
    "print(\"Spearman相关系数:\", correlation)\n",
    "# print(\"p值:\", p_value)\n",
    "#前13名计算召回率=精度：\n",
    "count = sum(1 for i in predicted_labels if i < topk)\n",
    "print(\"Precision or recall:\", count/topk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
